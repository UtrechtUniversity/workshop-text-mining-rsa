[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Text Mining with I-Analyzer & R",
    "section": "",
    "text": "Welcome\nText mining methods are quickly gaining popularity among researchers, including those from Law, Economics and Governance (LEG) disciplines. Text mining exploits sizeable collections of digitized texts for automatic analysis, using software such as R or Python. For example, newspaper articles have been to measure economic sentiments and digitized court records to analyze the evolution of jurisdiction.\nIn this course, we will get you up to speed with a simple workflow for data-driven research using digitized text collections, and learn to reflect on the pros and cons of using methods like these. We will start with selecting relevant texts in iAnalyzer and creating a dataset based on your own research question. After an introduction to R, you will gradually learn to analyze your dataset. This also allows you to develop general skills in R that are becoming ever more useful in the complex analysis of data. By the end of the course, you will be able to answer a simple research question based on your dataset, and to use an ‘Open Science’ approach to report on your workflow and the pros and cons of text-mining.\nAfter completing this module, students will be able to:\n\nprocess substantial datasets containing textual information;\nselect subsets of digitized text collections such as newspapers for analysis;\nimport and harmonize textual data in R;\nvisualize textual data using n-grams and relations between words;\nautomatically identify themes and subjects within textual data (topic modeling);\nreflect on the benefits and drawbacks of using text-mining methods for research."
  },
  {
    "objectID": "course-outline-and-schedule.html",
    "href": "course-outline-and-schedule.html",
    "title": "Course Outline & Schedule",
    "section": "",
    "text": "The course consists of 5 instructional meetings. Each meeting involves some preparation beforehand and homework afterwards. The preparation and homework will be necessary to ensure an efficient use of lesson time and you will be building up your final assignment as you go.\nAs you progress through the course, you can reach out to the instructors during the Walk-In Hours of Research Data Management Support with any questions or issues. The Walk-In Hours take place every Monday from 15:00 to 17:00 at the University Library in the Science Park. However, one instructor will be available at the University Library in the city center (in the seating area near the Digital Humanities Lab) and you are welcome free to request a meeting online (via MS Teams) during these hours as well.\nYou can also contact the course coordinator, Neha Moopen, by email at n.moopen@uu.nl\n\nMeeting 1Meeting 2Meeting 3Meeting 4Meeting 5\n\n\nIn this meeting, we will introduce students to I-Analyzer. I-Analyzer is an online text and data mining application developed by the Digital Humanities Lab at Utrecht University. We will work on the Times Newspapers corpus in iAnalyzer.\nWe will then proceed with exploring how you can process the corpora in I-Analyzer. This will include:\n\nSearching and filtering the corpus, as well as visualizing the results.\nCreating subsets of the data (corpus) and exporting it for further analysis in R.\n\nHomework\n\nComplete the exercise started in class.\nConduct a new query in I-Analyzer and export the results.\n\n\n\nPreparation\n\nInstall R & RStudio.\nWatch a video introducing R & RStudio.\n\nSession\nThe day of the course will be an introduction to R. This will include:\n\nR Syntax & Data Types\nVectors in R\nData Structures\nMissing Data\nIndexing Vectors & Lists\nIndexing a Data Frame\n\nNote that we do not cover programming techniques such as if statements, functions, loops. The aim of this session is to familiarize students with R and present some basic data wrangling operations.\nHomework\n\nComplete the Base R exercises started in class.\n\n\n\nPreparation\nReview recommended literature and provide a small presentation (possibly in groups) on selected papers.\nSession\nThis meeting will start off with a guest lecture from prof. dr. Femke van Esch who is based at the Utrecht School of Governance. The topic will not be text mining in and of itself, but a broader discussion of the opportunities provided by computational methods and techniques in LEG disciplines.\nThereafter, we will will dive into the presentations and discussion of literature. The final assignment includes a reflection assignment, the discussion here can provide a basis for that.\nHomework\nMake a start on your reflection assignment with some rough notes.\n\n\nPreparation\n\nInstall the necessary packages required for the TidyText session: tidyverse & tidytext\nCreate a folder structure for the next session, place the necessary files in the appropriate locations.\n\nSession\nThe meeting is where we will dive into text-mining with R. This will include:\n\nimporting and tidying textual data in R (tidy text format)\nsentiment analysis\nanalyzing word and document frequency (tf-idf)\ncalculating and visualizing relationships between words (n-grams and correlations)\nidentifying themes and subjects within textual data (topic modeling)\n\nHomework\n\nComplete the text mining exercises in R that we started during the session. If you get stuck, the instructors are available for help on Mondays from 15:00-17:00 either online or at the University Library in the City Center.\n\n\n\nPreparation\n\nFinalize your research question and dataset for final assignment, if you haven’t done so already.\nWatch a video on R Markdown.\n\nSession\nStudents will have already been working in R Markdown documents for Day 2 & Day 3, but we will take a step further to add prose/text between the code and ‘knit’ or render the R Markdown file to pdf or HTML. This is a fully reproducible workflow which weaves together text, code, results into one output file.\nThe resulting text-mining report can be eventually be submitted for grading, along with the reflection report.\nHomework\nWork on the final assignment.\n\n\n\nIn additional to the 5 instructional meetings, we can reserve an optional meeting(s) 6 & 7 in case they are required to refresh some concepts or if additional support is required for the assignments. This can be decided with the students and instructors as we go through the course."
  },
  {
    "objectID": "ianalyzer-preparation.html",
    "href": "ianalyzer-preparation.html",
    "title": "Preparation",
    "section": "",
    "text": "No preparation is required for this meeting."
  },
  {
    "objectID": "ianalyzer-session.html#slides",
    "href": "ianalyzer-session.html#slides",
    "title": "Text Mining with iAnalyzer",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "ianalyzer-session.html#exercise",
    "href": "ianalyzer-session.html#exercise",
    "title": "Text Mining with iAnalyzer",
    "section": "Exercise:",
    "text": "Exercise:\nBefore starting, create a folder for this course and a subfolder for today’s session in an accessible location on your computer.\nTo perform a first search, follow these steps:\n\nGo to the iAnalyzer website: https://ianalyzer.hum.uu.nl/\n\nLogin with your solisID\nPerform a search with these criteria:\n\n\n\n\n\n\n\n\n\nCorpus\nTimes (select at the top left from the dropdown menu ‘corpora’)\n\n\nQuery\n‘European Union’\n\n\nPublication Date\nFrom 1945 onwards\n\n\nOCR confidence\n80-100 (explanation of the OCR score is at the bottom of the page)\n\n\nPage Type\nStandard\n\n\nOn Front Page\nYes (So leave ‘false’-box unchecked)\n\n\nPublication Date\nFrom 1945 onwards\n\n\nCategory\nNews\n\n\nIllustration\nNo (So leave all boxes unchecked)\n\n\n\n\n\nCheck out the different visualizations (in the top, right next to the header ‘Filters’)\nDownload the data in a CSV format (At the top of the page, “download CSV”). Make sure that before you download the file, you click on the small gears icon next to the download button, click “show default fields”, and check the box “content”. Otherwise, your downloaded file will not contain the actual text of the articles.\n\n\nCheatsheet for Search Queries\n\n\n\nOperator\nDescription\nExample/Explanation\n\n\n\n\n+\nmeans AND\nbank +assets\n\n\n|\nmeans OR\nbank |assets -> Note that OR is already the default way to combine search terms, so bank assets would be sufficient in this example.\n\n\n-\nmeans NOT\nbank -assets\n\n\n” ”\nentire phrase\nallows the search for an entire phrase: “the assets of the bank”\n\n\n*\nwildcard\nA wildcard for any number of characters, e.g. bank* will match banking, banks, banked, etc. The wildcard is only allowed at the end of a word, and cannot be used with phrases (between ” quotes).\n\n\n~N\nfuzzy search\nDescribes fuzzy search. When placed after a term this signifies how many characters are allowed to differ. So bank~1 also matches bang, sank, dank etc. When placed after a phrase, this signifies how many words may differ\n\n\n\n\n\nExplanation OCR scores\nOCR stands for “Optical character recognition” and it is a key tool for text mining. OCR uses machine learning to extract words and lines of text from scans and images, which can then be used to perform quantitative text analysis or natural language processing.As you can imagine, dependent on the image/scan quality, the writing (machine vs. hand-written) etc., the automated recognition of characters will not be perfect. The OCR score is a metric used to quantify the accuracy of the text extraction (in other words, the score reflects how likely it is that there are errors in the text extraction)"
  },
  {
    "objectID": "ianalyzer-homework.html",
    "href": "ianalyzer-homework.html",
    "title": "Homework",
    "section": "",
    "text": "As homework, you are required to:\n1. Complete the I-Analyzer exercise conducted during the session.\nThis is if you didn’t already finish it during the meeting. You will be required to show the exported dataset as part of the grading of the course.\n2. Create your own text mining dataset using I-Analyzer.\nThis exercise will form the basis of your final assignment.\n\nThink about a research question you would like to answer using the Times Newspapers corpus in I-Analyzer.\nConstruct a query in line with your research question and document it for reference and reproducibility. You can refer to the cheatsheet for building search queries on the previous page.\nRun the query in I-Analyzer.\nExport the results as a csv file and save it in an accessible location.\n\nDon’t forget the preparation for the next session!"
  },
  {
    "objectID": "r-preparation.html#install-r-rstudio",
    "href": "r-preparation.html#install-r-rstudio",
    "title": "Preparation",
    "section": "Install R & RStudio",
    "text": "Install R & RStudio\n\nPersonal LaptopUtrecht University laptop\n\n\nBy personal laptop, we mean a laptop where you have administrator rights.\n\nInstall R from https://cran.rstudio.com/\nInstall RStudio from https://www.rstudio.com/products/rstudio/download/#download (if you are unsure about which link to click, you should probably use the top list with “installers”. From that, choose the installer for your operating system.)\n\n\n\n\nOpen Software Center\nInstall “R for Windows”\n\nInstalling R packages on a Utrecht University laptop can be hard. This is because R packages are installed on a so-called ‘mounted’ drive. This causes problems with the performance. Check the installation:\n\nOpen RStudio\nLook for a window that says “Console” (probably bottom left of your screen).\nWrite the following line of code in your console: .libPaths()\nHit ‘enter’.\n\nIf the resulting path is a local address (e.g. starting with C:/), you are fine and can skip the next few steps. If it starts with // you are installing packages on a mounted drive. This will affect the performance, so you will want to correct that:\n\nCreate a folder R-packages on a local drive.\nCopy the location to that drive. For example: C:/Users/User/R-packages\nRun the following line of code in the console: file.edit(file.path(“~”, “.Rprofile”))\nRStudio now opens an editor window.\nPaste .libPaths(“C:/Users/User/R-packages”) (or whatever path you copied in 8.) in the editor, and save the file.\nRestart your R session.\nAt the window on the bottom right of your screen, select the tab “packages”\nClick on “install”. A window opens. In this window, is Install to library pointing to C:/Users/User/R-packages? (or whatever path you copied in 8.)."
  },
  {
    "objectID": "r-preparation.html#explore-r-rstudio",
    "href": "r-preparation.html#explore-r-rstudio",
    "title": "Preparation",
    "section": "Explore R & RStudio",
    "text": "Explore R & RStudio\nWatch the following 7-minute video to get an idea of how R & RStudio works. Try to follow along on your own computer! The slides are also available for reference.\n\nVideo\n\n\n\n\n\n\n\n\n\n\n\nSlides\nFor this part, just consider the slides 12 - 16.\n\n\n\n\n\n\n\n\nUse this link to open slides in a new tab (refer to slide #12)"
  },
  {
    "objectID": "r-session.html#setup",
    "href": "r-session.html#setup",
    "title": "Base R",
    "section": "Setup",
    "text": "Setup\nBefore we get started, we need two R Markdown files to work with. Follow the instructions below:\n\nCreate a folder called base-r on your computer.\nCreate an empty R Markdown file for coding along with the instructor’s examples. Open RStudio and click through File -> New File -> R Markdown -> give your file a title -> select HTML as output -> save the .Rmd file in the base-r folder.\nDownload the R Markdown file that you will use for the exercises baseR_exercises.Rmd and save it in the base-r folder."
  },
  {
    "objectID": "r-session.html#lets-get-started-what-is-r-and-rstudio",
    "href": "r-session.html#lets-get-started-what-is-r-and-rstudio",
    "title": "Base R",
    "section": "Let’s get started: What is R and RStudio?",
    "text": "Let’s get started: What is R and RStudio?\nThis describes the basic idea of R and RStudio and how to work in the RStudio environment.\n\nSlides\nFor this part, just consider the slides 12 - 16.\n\n\n\n\n\n\n\n\nUse this link to open slides in a new tab (refer to slide #12)\n\n\nCode Chunk\nThis is an example of a so-called code chunk in which you write the actual code.\n\n#let's define x \nx <- 6\nx\n\ny <- \"apple\"\ny\n\nYou can insert chunks into your R Markdown file in the following ways:\n\nusing a keyboard shortcut Ctrl + Alt + I on Windows or Cmd + Option + I on MacOS\nusing the Add Chunk button in the editor toolbar\ntyping the chunk delimiters {r} and directly into the file\n\nYou can read more about code chunks on the R Markdown website.\n\n\nVideo"
  },
  {
    "objectID": "r-session.html#r-syntax-data-types",
    "href": "r-session.html#r-syntax-data-types",
    "title": "Base R",
    "section": "R Syntax & Data Types",
    "text": "R Syntax & Data Types\nNow we go a step further in the “language” of R and we learn how to assign variables and math functions, and what kind of different data types there are. We create vectors and lists. Just start at slide 18 and do the exercises as they come.\n\nSlides\n\n\n\n\n\n\n\n\nUse this link to open slides in a new tab (refer to slide #18)\n\n\nCode Chunk\n\n#let's define x \nx <- 1\nx\n\nx*3\n\ny <- x + 2\nlog2(y)\n\n\n\nVideo\n\n\n\n\n\n\n\n\n\n\n\nExercise 1\n\nDo the following calculation in R: one plus five, divided by nine.\nAssign the result of the calculation to a variable.\nTest if the result of your calculation is larger than 1.\nRound off the result to 1 decimal. Tip: use the Maths Functions section of the Base R cheat sheet!\n\n\n\nSolution 1\n\nSlides\n\n\n\n\n\n\n\n\nUse this link to open slides in a new tab (refer to slide #27)\n\n\nVideo"
  },
  {
    "objectID": "r-session.html#vectors",
    "href": "r-session.html#vectors",
    "title": "Base R",
    "section": "Vectors",
    "text": "Vectors\nHere, we create vectors and learn what a vector actually is! Just start at slide 28 and do the exercises as they come.\n\nSlides\n\n\n\n\n\n\n\n\nUse this link to open slides in a new tab (refer to slide #28)\n\n\nCode Chunk\n\nc(1,2,3)\nc(\"a\", \"b\", \"c\")\nc(T, TRUE, F)\n\nc(TRUE, \"a\", 3)\n\n# in mathematical operations\n\np <- 1:5\np\nmean(p)\np*2\n\nq <- 5:1\nq\n\np*q\n\n\n\nVideo\n\n\n\n\n\n\n\n\n\n\n\nExercise 2\nMeet Ann, Bob, Chloe, and Dan.\n\nMake a character vector with their names, using the function c(). Save the vector as “name”.\nHow old are Ann, Bob, Chloe, and Dan? You decide! Design a numeric vector with their respective ages. Save it as “age”.\nWhat is their average age? Use a function in R to calculate this. Tip: use the Maths Functions section of the Base R cheat sheet!\n\n\n\nSolution 2\n\nSlides\n\n\n\n\n\n\n\n\nUse this link to open slides in a new tab (refer to slide #38)\n\n\nVideo"
  },
  {
    "objectID": "r-session.html#data-structures",
    "href": "r-session.html#data-structures",
    "title": "Base R",
    "section": "Data Structures",
    "text": "Data Structures\nHere, we look at different data structures. Can we combine vectors, and lists? And how can we come up with a data frame? This will all be answered here. Just start at slide 41 and do the exercises as they come.\n\nSlides\n\n\n\n\n\n\n\n\nUse this link to open slides in a new tab (refer to slide #41)\n\n\nCode Chunk\n\n# vectors\n# two vectors: name and age from exercise before\nname\nage\n\n#combine vectors to a unidimensional vector:\nc(name,age)\n\n#combine vectors to multidimensional list:\nlist(name,age)\n\n#combine vectors to twodimensional data frame:\ndata.frame(name,age)\n\n#factors\n\ncountry <- c(\"UK\", \"USA\", \"USA\", \"UK\")\nfactor(country)\n\n#usually as column in a data frame: a categorial variable\n\ndf <- data.frame(name, age, country = factor(country))\ndf\n\nsummary(df)\n\n\n\nVideo\n\n\n\n\n\n\n\n\n\n\n\nExercise 3\n\nCreate a vector country containing four countries (use at least one duplicate!).\nCreate a data frame combining name, age, and country, and save it as df.\nCheck your dataframe with the function summary(). Does it contain a factor?\nMake sure your column country is a factor, and confirm this with summary().\nCreate a list with your vectors name and age, and save it as mylist.\n\n\n\nSolution 3\n\nSlides\n\n\n\n\n\n\n\n\nUse this link to open slides in a new tab (refer to slide #48)\n\n\nVideo"
  },
  {
    "objectID": "r-session.html#missing-data",
    "href": "r-session.html#missing-data",
    "title": "Base R",
    "section": "Missing data",
    "text": "Missing data\nWhat if for a data subject you do not have the data? How can you handle that in a data frame? We check here. Just start at slide 53 and do the exercises as they come.\n\nSlides\n\n\n\n\n\n\n\n\nUse this link to open slides in a new tab (refer to slide #53)\n\n\nCode Chunk\n\n# Data is not available: NA\ndf$pet <- factor(c(\"cat\", \"none\", \"fish\", NA))\ndf$pet\n\n\n\nVideo\n\n\n\n\n\n\n\n\n\n\n\nExercise 4\nPredict the results before you run the code. Does the real answer make sense to you?\n\n\nSolution 4\n\nSlides\n\n\n\n\n\n\n\n\nUse this link to open slides in a new tab (refer to slide #57)"
  },
  {
    "objectID": "r-session.html#indexing-vectors-and-lists",
    "href": "r-session.html#indexing-vectors-and-lists",
    "title": "Base R",
    "section": "Indexing vectors and lists",
    "text": "Indexing vectors and lists\nHere we explain how you get to one specific value or column or data entry in your vector, or list. Just start at slide 66 and do the exercises as they come.\n\nSlides\n\n\n\n\n\n\n\n\nUse this link to open slides in a new tab (refer to slide #66)\n\n\nCode Chunk\n\n# in vector name:selecting by position\nname\nname[2]\nname[1:3]\nname[c(2,2,1)]\n\n# in vector age: selcting by value\nage\nage[age > 40]\nage > 40\n\nname\nname[name %in% c(\"Chloe\", \"Ann\", \"Evie\")]\n\n# selecting from list\nmylist\n# selecting a list element\nmylist[1]\n# subselecting in the list element\nmylist[1][2]\n# selecting the content of a list element\nmylist[[1]]\n# subselecting in the content of a list element\nmylist[[1]][[2]]\n\n\n\nVideo\n\n\n\n\n\n\n\n\n\nStart the video at\n\n\nExercise 5\n\nReturn only the first number in your vector age.\nReturn the 2nd and 4th name in your vector name.\nReturn only ages under 30 from your vector age.\nReturn the name “Chloe” from your list mylist (see exercise 3).\n\n\n\nSolution 5\n\nSlides\n\n\n\n\n\n\n\n\nUse this link to open slides in a new tab (refer to slide #76)\n\n\nVideo"
  },
  {
    "objectID": "r-session.html#indexing-a-data-frame",
    "href": "r-session.html#indexing-a-data-frame",
    "title": "Base R",
    "section": "Indexing a data frame",
    "text": "Indexing a data frame\nHere we explain how you get to one specific value or column or data entry in your data frame. Just start at slide 80 and do the exercises as they come.\n\nSlides\n\n\n\n\n\n\n\n\nUse this link to open slides in a new tab (refer to slide #80)\n\n\nCode Chunk\n\n#indexing columns\n# by position\ndf[,2]\n\n# by name as a character string\ndf[, \"age\"]\n\n# by name, as an object\ndf$age\n\n#indexing rows\n# by position\ndf[2,]\n\n# by content\ndf[df$name =\"Bob\",]\n\n#combining rows and columns\ndf[df$name = \"Bob\", \"age\"]\n\n\n\nVideo\n\n\n\n\n\n\n\n\n\nStart the video at\n\n\nExercise 6\nBefore you start, please run this code:\n\nrm(name,age,country)\n\n\nFrom your dataframe df, return the entries for everyone living in a country of your choice.\nReturn only the names of everyone in your data frame df under 40. (Hint: what information should you use for row indexing? What information should you use for column indexing?)\nReturn the columns name and age together.\n\n\n\nSolution 6\n\nSlides\n\n\n\n\n\n\n\n\nUse this link to open slides in a new tab (refer to slide #89)\n\n\nVideo"
  },
  {
    "objectID": "r-session.html#recap",
    "href": "r-session.html#recap",
    "title": "Base R",
    "section": "Recap",
    "text": "Recap\n\nSlides\n\n\n\n\n\n\n\n\nUse this link to open slides in a new tab (refer to slide #92)\nYou made it! This was the basic introduction to R to get you started. If you want to continue with the slides, feel free to do so :)"
  },
  {
    "objectID": "r-homework.html",
    "href": "r-homework.html",
    "title": "Homework",
    "section": "",
    "text": "As homework, you are required to:\n1. Complete the Base R exercises conducted during the session.\nThis is if you didn’t already finish it during the meeting. Use the videos and slides as a reference for completing the exercises.\nNote that you will be required to submit the completed exercises: baseR_exercises.Rmd, as part of the grading of the course.\nIf you get stuck, don’t hesitate to reach out to the instructors during the Walk-In Hours of Research Data Management Support. The Walk-In Hours take place every Monday from 15:00 to 17:00 at the University Library in the Science Park. However, one instructor will be available at the University Library in the city center (in the seating area near the Digital Humanities Lab) and you are welcome free to request a meeting online (via MS Teams) during these hours as well.\nDon’t forget the preparation for the next session!"
  },
  {
    "objectID": "lecture-and-literature-preparation.html#reflection-assignment",
    "href": "lecture-and-literature-preparation.html#reflection-assignment",
    "title": "Preparation",
    "section": "Reflection Assignment",
    "text": "Reflection Assignment\nFamiliarize yourself with the Reflection Assignment for REBO Skills Academy Modules. This can be found towards the end of the syllabus. Consider the questions you will be asked to reflect on and keep them in mind as you review literature and listen to the guest lecture in the next session."
  },
  {
    "objectID": "lecture-and-literature-preparation.html#literature-review",
    "href": "lecture-and-literature-preparation.html#literature-review",
    "title": "Preparation",
    "section": "Literature Review",
    "text": "Literature Review\n\nReview the recommended literature relevant to text-mining in your discipline.\nSearch for one additional article/chapter on text-mining in your discipline. Bonus points if you manage to find an article/chapter that involved text mining using R.\nSummarize the literature in one PowerPoint slide per article. You should end up with 4 PowerPoint slides (at minimum).\nBring your slides to the next meeting. Note that you will also be asked to submit the slides as part of your final assignment.\n\n\nLAW:\n\nDyevre, A. (2021) ‘Text-mining for Lawyers: How Machine Learning Techniques Can Advance our Understanding of Legal Discourse’, Erasmus Law Review, 14(1). https://ssrn.com/abstract=3957098 (Accessed: 2 May 2023).\nWyner, A., Mochales-Palau, R., Moens, M.F., and Milward, D. (2010) ‘Approaches to Text Mining Arguments from Legal Cases’, in Francesconi, E., Montemagni, S., Peters, W., and Tiscornia, D. (eds) Semantic Processing of Legal Texts, Lecture Notes in Computer Science, vol 6036. Springer, Berlin, Heidelberg, pp. 42-56. https://doi.org/10.1007/978-3-642-12837-0_4 (Accessed: 2 May 2023).\nFeinerer, I. and Hornik, K. (2008) ‘Text mining of supreme administrative court jurisdictions’, in Data Analysis, Machine Learning and Applications: Proceedings of the 31st Annual Conference of the Gesellschaft für Klassifikation eV, Albert-Ludwigs-Universität Freiburg, March 7–9, 2007, Springer Berlin Heidelberg, pp. 569-576. https://doi.org/10.1007/978-3-540-78246-9_67\n\n\n\nECONOMICS/FINANCE:\n\nSiegel, M. (2018) ‘Text Mining in Economics’, in Hoppe, T., Humm, B., and Reibold, A. (eds) Semantic Applications. Springer Vieweg, Berlin, Heidelberg, pp. 109-126. https://doi.org/10.1007/978-3-662-55433-3_5 (Accessed: 2 May 2023).\nGentzkow, M., Kelly, B., and Taddy, M. (2019) ‘Text as Data’, Journal of Economic Literature, 57(3), pp. 535-574. https://doi.org/10.1257/jel.20181020 (Accessed: 2 May 2023).\nGupta, A., Dengre, V., Kheruwala, H.A. et al. (2020) ‘Comprehensive review of text-mining applications in finance’, Financial Innovation, 6(1), 39. https://doi.org/10.1186/s40854-020-00205-1 (Accessed: 2 May 2023).\n\n\n\nGOVERNANCE/POLICY:\n\nGyódi, K., Nawaro, Ł., Paliński, M. et al. (2023) ‘Informing policy with text mining: technological change and social challenges’, Quality & Quantity, 57, pp. 933-954. https://doi.org/10.1007/s11135-022-01378-w (Accessed: 2 May 2023).\nAbu-Shanab, E., and Harb, Y. (2019) ‘E-government research insights: Text mining analysis’, Electronic Commerce Research and Applications, 38, 100892. https://doi.org/10.1016/j.elerap.2019.100892 (Accessed: 2 May 2023).\nCogburn, D., 2020. Big data analytics and text mining in internet governance research: Computational analysis of transcripts from 12 years of the internet governance forum. Researching Internet Governance: Methods, Frameworks, Futures, p.214."
  },
  {
    "objectID": "lecture-and-literature-session.html#literature-discussion",
    "href": "lecture-and-literature-session.html#literature-discussion",
    "title": "Lecture & Literature",
    "section": "Literature Discussion",
    "text": "Literature Discussion\n\nGet into subgroups where all the LEG disciplines are represented.\nPresent your slides summarizing literature from your discipline to others in your groups."
  },
  {
    "objectID": "lecture-and-literature-session.html#reflection",
    "href": "lecture-and-literature-session.html#reflection",
    "title": "Lecture & Literature",
    "section": "Reflection",
    "text": "Reflection\nConsider the following questions when reflecting on the literature (across disciplines):\n\nWhat stood out to you in the literature? What did you notice that was really interesting or worthwhile to mention?\nWhat were the concepts or techniques that you did not fully understand?\nHow can text-mining help you understand important quetions in LEG disciplines? What does it add to the method and techniques that are currently in use?\nIf you were able to analyze all text eveywhere in your discipline, what would you be interested in learning?"
  },
  {
    "objectID": "lecture-and-literature-homework.html",
    "href": "lecture-and-literature-homework.html",
    "title": "Homework",
    "section": "",
    "text": "As homework, you can start working on a draft of your reflection assignment, which can be found towards the end of the syllabus. Focus on writing up some notes on the literature you’ve summarized so for. Again, this is only a draft version which you will finalize at the end of the course - but you can already make some progress in this week.\nDon’t forget the preparation for the next session!"
  },
  {
    "objectID": "text-mining-preparation.html#install-load-packages",
    "href": "text-mining-preparation.html#install-load-packages",
    "title": "Preparation",
    "section": "Install & Load Packages",
    "text": "Install & Load Packages\nLet’s be sure you have all the needed packages installed and that loading them into RStudio works fine.\nThese are two distinct operations: installing means downloading and installing all the files related to a package in your computer and this is usually a one-time operation, while loading a package means making the package’s functions and other features ready to be used in your R session.\nThe latter is something you need to do every time you start (or re-start) your R session. To install and load packages we use the R functions install.packages() and library(), respectively. Remember that install.packages() requires its arguments to be specified between double quotes, while library() accepts both double quotes or the plain name of the package.\nLet’s install the packages:\n\ninstall.packages(\"tidyverse\")\ninstall.packages(\"tidytext\")\ninstall.packages(\"wordcloud\")\n\nAnd load them:\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.2.3\n\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\n\nWarning: package 'tibble' was built under R version 4.2.3\n\n\nWarning: package 'tidyr' was built under R version 4.2.3\n\n\nWarning: package 'readr' was built under R version 4.2.3\n\n\nWarning: package 'purrr' was built under R version 4.2.3\n\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\nWarning: package 'forcats' was built under R version 4.2.3\n\n\nWarning: package 'lubridate' was built under R version 4.2.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(tidytext)\n\nWarning: package 'tidytext' was built under R version 4.2.3\n\nlibrary(wordcloud)\n\nWarning: package 'wordcloud' was built under R version 4.2.3\n\n\nLoading required package: RColorBrewer\n\n\nA few words about the packages we are going to use:\n\n\ntidyverse: this is an “opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures”. Among the many tidyverse packages, we are going to use in particular:\n\n\nreadr: allowing to read data from files;\n\n\ndplyr: providing tools for data manipulation;\n\n\ntidyr: providing tools to make your data “tidy”;\n\n\nforcats: providing tools to solve problems with factors, and advanced R topic we would not spend much time talking about;\n\n\nggplot2: a system for creating graphics.\n\n\n\ntidytext: an R package for text mining based on the tidy data principles;\n\n\nwordcloud: a package to generate word cloud plots."
  },
  {
    "objectID": "text-mining-preparation.html#project-organization",
    "href": "text-mining-preparation.html#project-organization",
    "title": "Preparation",
    "section": "Project Organization",
    "text": "Project Organization\n1. Create a project in RStudio\n\nIn RStudio, click File -> New Project -> New Directory -> New Project.\nGive your project directory the following name: text-mining-in-r\nMake sure your project directory folder is created (as a subdirectory) in an accessible place on your system.\nSelect Open project in a new session.\n\n2. Create a project structure suited for reproducible work\n\nYou can generate a directory structure by running the following piece of code in your R console:\n\n\ndir.create(\"data\", recursive = TRUE)\ndir.create(\"lexicons\", recursive = TRUE)\ndir.create(\"docs\", recursive = TRUE)\ndir.create(\"results\", recursive = TRUE)\ndir.create(\"R\", recursive = TRUE)"
  },
  {
    "objectID": "text-mining-preparation.html#download-data",
    "href": "text-mining-preparation.html#download-data",
    "title": "Preparation",
    "section": "Download Data",
    "text": "Download Data\nDownload the data file linked below (right-click and select Save link as…) and place it in the data folder you just created:\nianalyzer_query.csv"
  },
  {
    "objectID": "text-mining-preparation.html#download-lexicon",
    "href": "text-mining-preparation.html#download-lexicon",
    "title": "Preparation",
    "section": "Download Lexicon",
    "text": "Download Lexicon\nDownload the lexicon file linked below (right-click and select Save link as…) and place it in the lexicons folder:\nNRC_lexicon.txt"
  },
  {
    "objectID": "text-mining-preparation.html#download-the-r-markdown-files",
    "href": "text-mining-preparation.html#download-the-r-markdown-files",
    "title": "Preparation",
    "section": "Download the R Markdown Files",
    "text": "Download the R Markdown Files\nDownload the following Rmd files (right-click and select Save link as…) and place these files in the R folder you just created.:\n\ntext_mining_with_R_notebook_empty.Rmd\ntext_mining_with_R_notebook_solutions.Rmd"
  },
  {
    "objectID": "text-mining-preparation.html#some-reading",
    "href": "text-mining-preparation.html#some-reading",
    "title": "Preparation",
    "section": "Some Reading",
    "text": "Some Reading\nRead the section Just before starting: tidyverse pipelines in the next page. It’s alright if you don’t understand everything but it will help you become more familiar with the syntax we will be using in the text mining session."
  },
  {
    "objectID": "text-mining-preparation.html#bonus-reading",
    "href": "text-mining-preparation.html#bonus-reading",
    "title": "Preparation",
    "section": "Bonus Reading",
    "text": "Bonus Reading\nThe Text Mining with R textbook is our go-to reference for the upcoming session. Take some time to skim through the case studies presented in the book:\n\nComparing Twitter archives\nMining NASA metadata\nAnalyzing usenet text\n\nAgain, it’s alright if you don’t understand everything. This is to familiarize you with the techniques we will be trying out in the next session and see how results of text mining techniques are presented and interpreted."
  },
  {
    "objectID": "text_mining_with_R.html#slides",
    "href": "text_mining_with_R.html#slides",
    "title": "Text Mining with R",
    "section": "Slides",
    "text": "Slides\nThe content of this chapter is also summarized in the following slides, if you need a quick refresher or overview:"
  },
  {
    "objectID": "text_mining_with_R.html#lets-get-started",
    "href": "text_mining_with_R.html#lets-get-started",
    "title": "Text Mining with R",
    "section": "Let’s Get Started!",
    "text": "Let’s Get Started!\n\nprint('Good luck everyone!')\n\n[1] \"Good luck everyone!\"\n\n\nWhen necessary, code and outputs blocks are followed by a line-by-line explanation of the operations executed in the code box:\n\n\n\n\n\nHow does it work?\n\n\n\n\n\n\nThe function print() displayed the custom message “Good luck everyone!” as an encouragement to all the students following the course.\n\n\n\n\n\nData Analysis Setup\nLet’s be sure you have all the needed packages installed and loaded into your R studio session. Remember that installing means downloading and installing all the files related to a package in your computer and this is usually a one-time operation. On the other hand, loading a package means making the package’s functions and other features ready to be used in your R session. This is something you need to do every time you start (or re-start) your R session. To install and load packages we use the R functions install.packages() and library(), respectively. Remember that install.packages() requires its arguments to be specified between double quotes, while library() accepts both double quotes or the plain name of the package.\nLet’s install packages\n\ninstall.packages(\"tidyverse\")\ninstall.packages(\"tidytext\")\ninstall.packages(\"wordcloud\")\n\nand load them:\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.2.3\n\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\n\nWarning: package 'tibble' was built under R version 4.2.3\n\n\nWarning: package 'tidyr' was built under R version 4.2.3\n\n\nWarning: package 'readr' was built under R version 4.2.3\n\n\nWarning: package 'purrr' was built under R version 4.2.3\n\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\nWarning: package 'forcats' was built under R version 4.2.3\n\n\nWarning: package 'lubridate' was built under R version 4.2.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(tidytext)\n\nWarning: package 'tidytext' was built under R version 4.2.3\n\nlibrary(wordcloud)\n\nWarning: package 'wordcloud' was built under R version 4.2.3\n\n\nLoading required package: RColorBrewer\n\n\nFew words about the packages we are going to use:\n\n\ntidyverse: this is an “opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures”. Among the many tidyverse packages, we are going to use in particular:\n\n\nreadr: allowing to read data from files;\n\n\ndplyr: providing tools for data manipulation;\n\n\ntidyr: providing tools to make your data “tidy”;\n\n\nforcats: providing tools to solve problems with factors, and advanced R topic we would not spend much time talking about;\n\n\nggplot2: a system for creating graphics.\n\n\n\ntidytext: an R package for text mining based on the tidy data principles;\n\n\nwordcloud: a package to generate word cloud plots.\n\n\n\n\nJust before starting: tidyverse pipelines\nGenerally speaking, in programming a function takes an input and some more information used to customize the function behavior (parameters), it runs some operation on the input according to the specified parameters, and it returns an output.\n\n\n\n\n\nIn data analysis, most of the times, you will find yourself in the situation where, in order to plot meaningful information or to extract meaningful statistical quantities from data, you need to “feed” the data to several different functions, collect their outputs, and use those results as input for other functions. For example, you may want first read the data from a file, then select two columns, count how many identical values are in those columns, and plot a histogram plot with those counts. Each of these operation will in practice performed by a single function. Assuming we want to apply 4 different functions to data, one after another (like in the previous example), our R code will look like something like this:\n\noutput1 <- func1(data, pars1)\noutput2 <- func2(output1, pars2)\noutput3 <- func3(output2, pars3)\noutput4 <- func4(output3, pars4)\n\nIn most of the cases we will be interested only in the final output (output4) and saving each output into a variable (as it happens in the previous block of code) would only waste memory space. To avoid that, we can use directly the output of a function as input for the next one in a nested structure:\n\noutput4 <- func4(func3(func2(func1(data,pars1),pars2),pars3),pars4)\n\nAt this point you probably noticed that the nested structure can be quite confusing, making us loosing track of what is the input and the additional parameters for each function, especially when we need to apply several functions to data one after another.\nTo simplify this kind of situations, the package tidyverse (once loaded) allows to use an alternative syntax called pipeline. In a pipeline, the input parameter of a function is NOT specified between the parenthesis of the function, like in plain R, but it appears OUTSIDE the function and it is followed by the “pipe” operator “%>%”. The pipe operator takes whatever is on its left-hand side and it passes it as an input for the code on the right-hand side. Confused? Look how the previous line of code becomes using the tidyverse pipeline:\n\noutput4 <- data %>% \n  func1(pars1) %>%\n  func2(pars2) %>%\n  func3(pars3) %>%\n  func4(pars4) \n\n\n\n\n\n\nHow does it work?\n\n\n\n\n\n\ndata is our first raw input. The pipe operator “$>$” takes it and uses it as an input for func1(pars1), where pars1 are additional parameters specified by the user. func1 will produce an output, but instead of storing this output into a variable, we immediatly “inject” it into another function using again the pipe operator. We repeat this operation four times, one time for each function we want to use, until we finish to perform all our operations. The result will be stored in the variable output4 as indicated by the operator “<-” at the beginning of the line.\n\n\n\n\nThe workflow and the final computed result of the previous two blocks of code (with plain R nested functions and tidyverse pipeline, respectively) is absolutely identical, but the tidyverse pipeline allows to write nested functions in a more concise, readable, easy to understand, and easy to modify way. Code using tidyverse pipelines is therefore less prone to errors, but it is ultimately up to you to choose your preferred syntax.\n\n\nReading data\nIn this session we will work with data that we already collected using the web application I-Analyzer. We obtained our data querying for “European Union” over the news of the Times Digital Archive between 1945 and 2010. The data are stored in a coma-separated-values (csv) file and we are going to convert them into an R DataFrame:\n\ndata_file_name <- '../data/ianalyzer_query.csv'\n\ndata_df <- read_delim(data_file_name, \n    delim = \";\", \n    escape_double = FALSE, \n    col_types = cols(`date-pub` = col_date(format = \"%B %d, %Y\"), \n        issue = col_integer()), trim_ws = TRUE)\n\nprint(nrow(data_df))\n\n[1] 1532\n\nprint(colnames(data_df))\n\n[1] \"author\"   \"category\" \"content\"  \"date-pub\" \"edition\"  \"issue\"    \"query\"   \n[8] \"title\"    \"volume\"  \n\n\n\n\n\n\n\nHow does it work?\n\n\n\n\n\n\nWe first load the library readr as it contains all sort of dunctions to read files into R DataFrames. We also load the library tidyverse as it contains a lot of useful functions for data manipulation and visualization. Furthermore it will allow us to use the tidyverse pipeline syntax;\n\n\nWe store the name of our dataset into the variable data_file_name. The name includes the realtive path of the data, as the code is supposed to be ran in the book folder of the provided material;\n\n\nWe read the data using read_delim(), a function to read coma separated files (csv) and more into R DataFrames. In this case, we obtained all the necessary arguments clicking on “Import Dataset” in the Environment window (top-right) of R studio. In particular, we gave instructions to convert the date of publication column (“date-pub”) data format (<month_str> <day>, <year>) into an R date object;\n\n\nWe finally printed the names of the columns in our DataFrame using colnames().\n\n\n\n\nWe now have all the data we need in a single R DataFrame, data_df, the starting point of our analysis. The information we are most interested in it’s stored in the “content” column. Let’s see how to extract this information.\n\n\nText Preprocessing\n\nTokenization\nAs we mentioned in the introduction, text data mostly comes unstructured and it is up to us to define a data structure suitable for the kind of text analysis we want to perform. We want our data structure to be comprehensive and to be easily manipulated according to our needs. Having this goal in mind, a possibility is to divide text into either characters, words, or paragraphs and to obtain a data set where each row contains one of these elements and all its relative information.\nThe process of dividing a string of text into meaningful units is called Tokenization and these meaningful units are called tokens. A token can be a word, a phrase, a paragraph, or a single character depending on the nature of our analysis. If, for example, we want just to explore how many times the name of a certain politician is mentioned in a speech, our tokens would probably be all the words of the speech and our analysis would consist on counting how many tokens are equal to the name of the politician.\nTo perform “good” text mining, not only we want to optimally “tokenize” our text as well as organize our tokens in a tidy way, quite literally! For the R community, “tidy” has a very specific meaning, i.e. structuring data sets to make them consistent, easy to work with, and easy to analyze. In our context it means having a single token per data frame row. R allows us to perform all these operations in few lines thanks to the library tidytext:\n\ntidy_content <- data_df %>% unnest_tokens(word, content, token=\"words\")\n\ntidy_content\n\n# A tibble: 1,549,578 × 9\n   author             category `date-pub` edition issue query title volume word \n   <chr>              <chr>    <date>     <lgl>   <int> <chr> <chr> <lgl>  <chr>\n 1 ['FROM A SPECIAL … ['News'] 1962-11-05 NA      55540 time… Euro… NA     from \n 2 ['FROM A SPECIAL … ['News'] 1962-11-05 NA      55540 time… Euro… NA     a    \n 3 ['FROM A SPECIAL … ['News'] 1962-11-05 NA      55540 time… Euro… NA     spec…\n 4 ['FROM A SPECIAL … ['News'] 1962-11-05 NA      55540 time… Euro… NA     corr…\n 5 ['FROM A SPECIAL … ['News'] 1962-11-05 NA      55540 time… Euro… NA     at   \n 6 ['FROM A SPECIAL … ['News'] 1962-11-05 NA      55540 time… Euro… NA     this \n 7 ['FROM A SPECIAL … ['News'] 1962-11-05 NA      55540 time… Euro… NA     junc…\n 8 ['FROM A SPECIAL … ['News'] 1962-11-05 NA      55540 time… Euro… NA     of   \n 9 ['FROM A SPECIAL … ['News'] 1962-11-05 NA      55540 time… Euro… NA     focus\n10 ['FROM A SPECIAL … ['News'] 1962-11-05 NA      55540 time… Euro… NA     on   \n# ℹ 1,549,568 more rows\n\n\n\n\n\n\n\nHow does it work?\n\n\n\n\n\n\nWe first load the library tidytext containing all the functions we need for text mining;\n\n\nWe use the tidytext function unnest_tokens() to split the text stored in the column content of our DataFrame data_df into words. First of all, we “feed” our raw data DataFrame to the function using the workflow syntax data_df %>%       unnest_tokens(...). This is equivalent to specifying the DataFrame as the first argument of the function, i.e. unnest_tokens(data_df, work,content,token=\"words\"). The other three arguments of the function specify the name of the column containing the tokens (word), the text that needs to be tokenized (the column content), and the kind of token (in our case, words).\n\n\n\n\nIn the code block above we use the function unnest_tokens() to tokenize the content of our DataFrame. A quick view of the resulting DataFrame tidy_content shows us that the content column is gone, while a column named word now appears at the end of the DataFrame. How do we know which word belongs to which content AFTER tokenization? Is that information lost? The content in your raw data DataFrame has a unique identifier associated with it, in our case the identifier is stored in the column issue. As the column issue is STILL present in tidy_content, each word, or token, has therefore a unique identifier specifying from which content it came from.\n\n\nCleaning up data\nCleaning up data is an essential step in any data analysis process. Before you can analyze a dataset, you need to ensure that the data is accurate (data values are correct and free from errors), complete, and consistent (uniform and following a standard format or structure). Data cleaning is the process of identifying and correcting or removing errors, inconsistencies, and inaccuracies from a dataset. The output of the I-Analyzer data (the csv file we are using for our analysis) is already a consistent data set, however, every time one of our programs performs some sort of operation on the data set, errors may occur. The absolute nightmares of every data scientist are missing or undefined values, these usually appear in our datasets as “NA” (Not Available) or “NaN” (Not a Number). NA is used in R (and some other programming languages) to represent missing or undefined values in data. You will find NaN more often in Python or Javascript programming. In general NA is used for non-numeric data types like characters and NaN for numeric data types, but the most important thing to remember is that you want to find out if your data set contains any of these values as they may severely affect your data analysis results. Does our DataFrame contain any NA values in the issue column? Let’s find out!\n\nare_there_na <- any(is.na(tidy_content$issue))\nif (are_there_na) {\n  print('WARNING! There are NA in your data!')\n} else {\n  print('Lucky you! I could not find any NA in your data (yet...)!')\n}\n\n[1] \"WARNING! There are NA in your data!\"\n\n\n\n\n\n\n\nHow does it work?\n\n\n\n\n\n\nThe function is.na(data_df) scans all the values of our DataFrame and returns another DataFrame made of boolean variables, i.e. True or False depending if the scanned value is a NA or not, respectively. We then apply the function any() to this boolean DataFrame to check if there is AT LEAST one TRUE value and, therefore, one NA value. If so, any() will return TRUE, FALSE otherwise. This result is stored in the variable are_there_na that, indeed, will be TRUE or FALSE depending if there is at least one NA in our data set or nor;\n\n\nWe use a conditional statement IF … ELSE … to print two messages depending on the value of the variable are_there_na.\n\n\n\n\nWe found NA in our dataset. Nothing to worry about, this is the common case, so common that there are plenty of functions to clean up our data. How shall we proceed in this case? Usually, we would like to inspect the data to check what is missing and how to better replace those values. In our case, you can relatively easily find out that one of the issues’ identifier is missing. For simplicity, we will just exclude the issue from our data set using the function na.omit() that, indeed, removes all the rows containing NA returning a “clean” DataFrame:\n\ntidy_content <- tidy_content[!is.na(tidy_content$issue), ]\n\n\n\n\n\n\nHow does it work?\n\n\n\n\n\n\nThe syntax tidy_content[<rows>,<columns>] allows us to select specific rows and columns in our DataFrame, an operation called slicing. In place of  and  we can put numerical indices specifying single or ranges of rows/columns or we can use a conditional statement on specific rows/columns. In this case, we select all the DataFrame rows whose value in the issue column is NOT Na. We do not specify any condition on columns, this means we will select ALL the DataFrame columns;\n\n\n\n\nLet’s check if there are still NA in our data set:\n\nare_there_na <- any(is.na(tidy_content$issue))\nif (are_there_na) {\n  print('WARNING! There are NA in your data!')\n} else {\n  print('Lucky you! I could not find any NA in your data (yet...)!')\n}\n\n[1] \"Lucky you! I could not find any NA in your data (yet...)!\"\n\n\n\nA second problem of unstructured data is that it can be very “noisy”, i.e. it can contain a lot of irrelevant information. Actually, the most common words in a text are words that have very little meaning, such as “the”, “and”, “a”, etc. These words are referred to as stop words and removing stop words from text (in a way or another) is a fundamental step of text mining:\n\ndata(stop_words)\n\ntidy_clean_content <- tidy_content %>% anti_join(stop_words)\n\ntidy_clean_content\n\n# A tibble: 801,754 × 9\n   author             category `date-pub` edition issue query title volume word \n   <chr>              <chr>    <date>     <lgl>   <int> <chr> <chr> <lgl>  <chr>\n 1 ['FROM A SPECIAL … ['News'] 1962-11-05 NA      55540 time… Euro… NA     spec…\n 2 ['FROM A SPECIAL … ['News'] 1962-11-05 NA      55540 time… Euro… NA     corr…\n 3 ['FROM A SPECIAL … ['News'] 1962-11-05 NA      55540 time… Euro… NA     junc…\n 4 ['FROM A SPECIAL … ['News'] 1962-11-05 NA      55540 time… Euro… NA     focus\n 5 ['FROM A SPECIAL … ['News'] 1962-11-05 NA      55540 time… Euro… NA     euro…\n 6 ['FROM A SPECIAL … ['News'] 1962-11-05 NA      55540 time… Euro… NA     37   \n 7 ['FROM A SPECIAL … ['News'] 1962-11-05 NA      55540 time… Euro… NA     women\n 8 ['FROM A SPECIAL … ['News'] 1962-11-05 NA      55540 time… Euro… NA     sat  \n 9 ['FROM A SPECIAL … ['News'] 1962-11-05 NA      55540 time… Euro… NA     round\n10 ['FROM A SPECIAL … ['News'] 1962-11-05 NA      55540 time… Euro… NA     conf…\n# ℹ 801,744 more rows\n\n\n\n\n\n\n\nHow does it work?\n\n\n\n\n\n\ndata(stop_words) loads a data set containing the most commonly used stop words in English language into the R environment. If you are in R studio you will now see in your Environment tab (the top-right one) a new DataFrame called stop_words;\n\n\nUsing the function anti_join() we select those rows (so words) in the DataFrame tidy_content that do NOT match the stop words. In other words, we created a new DataFrame (tidy_clean_content) removing all the stop words listed in stop_words from tidy_content.\n\n\n\n\nWith the few lines of code above, we cleaned up the tidy_content DataFrame from stop words. We perfomed basic data cleaning and our data set is now ready for some analysis.\n\n\n\nText Analysis\n\nCounting words\nAfter having tokenized and cleaned up our data, it’s time to perform the most basic text mining operation: count the number of times a certain word appears in our text. Even if conceptually it seems a quite trivial operation, by counting the frequency of each word in a text we can gain important insights about the general characteristics of the text. Furthermore, counting the frequency of specific words (i.e. how many times they occur per sentence/text/paragraph/document/etc) we can classify a text or define a model to identify the underlying topic or theme.\n\nword_count <- tidy_clean_content %>%\n  count(word) %>%\n  filter(n > 2000) %>%\n  mutate(word = reorder(word, n)) \n\nword_count_plot <-\n  word_count %>%\n  ggplot(aes(n, word)) +\n  geom_col() +\n  labs(y = NULL)\n\nword_count_plot\n\n\n\n\n\n\n\n\n\nHow does it work?\n\n\n\n\n\n\nWe “inject” our data as input of the function count() that, indeed, counts the identical entries of the column word. count() has an argument (name) that it is used to specify the name of the column where the count values will be stored. In this case, the name argument is omitted, therefore, by default the name of the column will be n. After using count(), the result is a DataFrame with two columns: the quantity that has been counted (word) and the result of the counting (n). There are a lot of words in our text and it would not be very convenient plotting all of them, so we apply the function filter() to select only those words with count (column n) larger than 2000. We also want words to be displayed in descending order of counts, to do that we use the function mutate() to substitute the column word with the same column (word) arranged by counts (n). To rearrange the words in order of counts we use the function reorder(). We store the result in the DataFrame word_count;\n\n\nTime to plot! We “feed” our data to the function ggplot(), in it we specify x and y axis to plot with aes(n, word) (aes stand for aesthetic). geom_col() specified that type of plot, in this case a column chard or a bar chart, and labs(y = NULL) removes the y-axis label (as it is quite obvious that we are visualizing words).\n\n\n\n\nLooking at the data, we are not surprised to see that the second most frequent word is “european” as we are analyzing data extracted from the “European Union” query. It is interesting to note that the third most frequent word is “vote”. This may suggest that all the times “European Union” was mentioned in the news, it was mostly related to elections.\nA very popular way to visualize the most common words in a sample of texts is a word cloud. A word cloud is a graphical representation of words where the size of each word indicates its frequency within a given data set. Visualizing our data using a word cloud is not particularly useful for quantitative analysis, but it can be very effective when communicating text mining results. Obtaining a word cloud plot in R is as easy as all the tasks we performed so far, we just need to download the package workcloud and use the function wordcloud() as follows:\n\nword_cloud_plot <-\n  word_count %>%\n  with(wordcloud(word, n, max.words = 1000))\n\n\n\nword_cloud_plot\n\nNULL\n\n\n\n\nWord classification and Sentiment Analysis\nCan we run a text analysis to evaluate if a sentence is sad or happy or if the general mood of a story is positive or negative? We actually can! and this may seem surprising as the mood or the sentiment related to a text seems to be something very subjective and highly affected by personal judgement and background. According to wikipedia, sentiment analysis (also known as opinion mining or emotion AI) is the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information. Sentiment analysis can become very complex, so in this session we will only perform a very basic sentiment analysis to grasp its main principles and workflow.\nSentiment analysis is based on the assumption that we can view a text as a combination of individual words and that, having assigned a sentiment to each individual word, the resulting sentiment of the text will be the sum of the sentiment of its individual words. For example, if most of the words in a text are positive, then the text can be classified as positive. Given this assumption, to perform sentiment analysis, we need a reference database of words in which a sentiment has been assigned to each word of the human vocabulary following specific conventions. Such a database is called lexicon.\nFor our simple analysis, we already downloaded a lexicon, the NRC emotion lexicon. The NRC Emotion Lexicon is a list of English words and their associations with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive). If an emotion is associated with a word, its score will be 1, and if not it will be 0.\n\nnrc_lexicon_df <- read.table(\"../lexicons/NRC_lexicon.txt\", header = FALSE, sep = \"\\t\", stringsAsFactors = FALSE, col.names = c(\"word\", \"emotion\", \"score\"))\n\njoy_words <- nrc_lexicon_df  %>% \n  filter(emotion == \"joy\" & score == 1)\n\n\n\n\n\n\nHow does it work?\n\n\n\n\n\n\nThe NRC emotion lexicon is located in the data folder and it is stored in a .txt file (even if technically it is a tab separated values file). To read its content into an R DataFrame we use the same procedure we used for our raw data;\n\n\nOnce we have stored the lexicon into the DataFrame nrc_lexicon_df we can filter only the “happy” words applying the function filter() and specifying that only words with emotion joy and score 1 are going to be considered. The result is stored into the DataFrame joy_words.\n\n\n\n\nIn the previous lines we read the lexicon into an R DataFrame and we extracted from it only the words associated with joy. Once we know which are the words associated with joy, we can count how many “joy words” there are in our text and see if the news associated with “European Union” can be classified as “joyful” or not.\n\nissue_df <- tidy_clean_content %>%\n  filter(`date-pub`>='2000-01-01' & `date-pub` < '2010-01-01') %>%\n  group_by(issue) %>%\n  reframe(words_per_issue = n(), date= `date-pub`) %>%\n  unique()\n\nissue_joy_df <- tidy_clean_content %>%\n  filter(`date-pub`>='2000-01-01' & `date-pub` < '2010-01-01') %>%\n  inner_join(joy_words) %>%\n  group_by(issue) %>%\n  reframe(joy_words_per_issue = n()) \n\nJoining with `by = join_by(word)`\n\nissue_tot_df <- merge(issue_df, issue_joy_df, by='issue')\n\n\n\n\n\n\nHow does it work?\n\n\n\n\n\n\nWe first apply the filter() function to our tidy_clean_content data set to select data between two specific dates in order to reduce the size of our data set. We then apply the function group_by(). On the surface, this function does not perform any action, but from now own any further operation in the pipeline will be performed on groups or rows. The way rows are grouped is determined by the argument of the function, in this case issue. The n() function counts the number of rows in the DataFrame and when applied after group_by(issue) it counts the number of rows (words) for groups of rows sharing the same value in the issue column, i.e. the number of words in each issue. The function reframe() changes the format of the output DataFrame creating two columns: words_per_issue containing the total number of words in each issue (result of n()) and date storing the date of publication. Finally, the function unique() will keep only unique values of the issue;\n\n\nWe repeat the same operation of the previous code, but this time we join our DataFrame with joy_words using the function `inner_join()’. In this way, only “happy words” will be selected from tidy_clean_content;\n\n\nWe finally merge the two previously created DataFrames so to have total number of words and total number of joy words per issue in the same DataFrame.\n\n\n\n\nThe starting point is always our cleaned data tidy_clean_content. We first apply a filter to select news in the decade 2000-2010, then we group our data according to the issue (remember, our data has been tokenized, so at the moment each row corresponds to a single word, not to a single news or issue) and we count the number of words per issue and the number of joy words per issue. The reason for this double computation is that we would like to compare the total number of joy words with the total number of words in each issue, to obtain in this way the percentage of joy words per issue.\n\npercent_of_joy_plot <-\n  issue_tot_df %>%\n  mutate(per_cent_joy=joy_words_per_issue/words_per_issue*100) %>%\n  ggplot(aes(x = date, y = per_cent_joy) )+\n  geom_col() +\n  labs(x = \"Date\", y = \"Joy words [%]\", title = \"Joyfulness about EU in 2000-2010\")\n\npercent_of_joy_plot\n\n\n\n\n\n\n\n\n\nHow does it work?\n\n\n\n\n\n\nTime to plot! We first add a column to our DataFrame for plotting convenience. We use the function mutate() to add the column per_cent_joy that, indeed, stores the percent of joy words per issue;\n\n\nWe use ggplot(aes()), geom_col(), and labs() to specify x and y axis in out plot, the type of plot, and the labels for our axes, respectively.\n\n\n\n\nInspecting the plot we can see that the percent of joy words never exceed the 8% and that there is a decreasing joy trend up to 2006 while values go back to a more average 4% level in 2007. 8% is a quite small value, what can we deduce from that? Text mining is a powerful technique, but like any other kind of statistics needs to be properly read according to the context. First of all, we would like to know if 8% is a particularly low value (as it seems like) or whether it is just the average level of joy associated with European Union news. We may even find out that the Times, by default, reports EU related news with a certain level of “coldness” so that 8% would represent a totally normal value in our sample. One of the main challenges in this case (and in general statistics) is establishing the proper level of “background joy level” or even the presence of a “joy bias” (a factor that can influence the results of an experiment in a particular direction, in our case low joy). Only determining these factors would allow us to obtain meaningful conclusions from our “joy” levels. A deep sentimental analysis of the sample is out of the scope of this course, but you may try yourself to figure out yourself ways to make our sentiment analysis conclusions more consistent.\nChecking the evolution of joy words over time is particularly interesting, but what about if we want just to quantify the percent of joy words in the entire corpus? Let’s just apply operations we already met before:\n\ndistinct_words <- tidy_clean_content %>%\n  distinct(word)\n\ntotal_dis_words <- nrow(distinct_words)\ntotal_dis_joy_words <- nrow(inner_join(distinct_words,joy_words,by='word'))\n\ntotal_joy <- (total_dis_joy_words/total_dis_words)*100\nprint(total_joy)\n\n[1] 0.5347957\n\n\nNot very high… it seems after all that Europen Union related news are not joyful at all!\n\n\nAnalyzing word and document frequency: tf-idf\nIn the previous sessions we have seen how to compute and display the number of times a term appears in a text. When we divide that number by the total number of words in that text we obtain a quantity called term frequency (tf). In text mining, a term is a unit of meaning that is used to represent a concept or an idea in a text and it may consist of more than one word. A term can be a word but the vice versa is not always true. “Big Data”, “Sentiment Analysis”, “Neural Network”, “Science Fiction” are all terms but, as you can see, they are made of two distinct words that considered singularly and depending on the context can have a total different meaning. In our case, for simplicity and unless specifically stated, we will use “term” and “word” to refer to single words (but be aware of the difference!). The term frequency quantifies how often a word occurs in a text and at first glance it may be considered as an indicator of the importance of that term in a text. One of the goals of text mining is trying to find a way (or several ways) to extract the topic (or the sentiment or any other information) from a text, but is the most frequent term in that text a good indicator of what the text is about? Is the most frequent term the most important word in the text? Well, in most of the cases it is not and this is because of two main reasons: 1) the most frequent terms are usually meaningless (e.g. stop words) and 2) the fact that a term is not used very often it does not necessarily imply that it is not important or representative for the entire text.\nIn the previous sessions we tried to deal with the first reason by filtering out stop words from our text. In this session we will see how to take into account those terms that are not that frequent in a text but that can still be important to deduce its general meaning/topic/sentiment. At the same time we will define a new statistical quantity that will allow us to filter out the most frequent meaningless words without “manually” removing them from our data.\nThe following equation defines the inverse document frequency (idf):\n\\[ idf(term) = log \\Bigg( {n_{documents} \\over n_{documents \\space containing \\space term}} \\Bigg) \\]\nThe idf is a function of the term we want to focus on, this means that we can assign a unique idf to every single term in our text. Assuming that we have a sample (a set) of different documents, we start computing the document frequency in the sample, i.e. the ratio between the number of documents containing our selected term and the total number of documents in the sample. We then invert this quantity (that is the reason why we talk about inverse document frequency) and we compute the natural logarithm of this inverted quantity. Why the logarithm? Applying the logarithmic function has two advantages: 1) it makes easy to “deal” with either very small or very big numbers (it makes these numbers easier to plot, for example) and 2) when computed on a ratio of two quantities a and b, the logarithm is positive if a > b and negative if b > a.\n\n\n\nidf\n\n\nIn our specific case, if our selected term is present in all the documents of our sample, then the argument of the logarithm is 1 and idf will be 0. If our selected term is present in less documents than the total number of documents in our sample, then idf will be positive. The less present our term is, the larger idf will be and vice versa (remember? We are dealing with inverse document frequency). Can idf be negative? It cannot because the number of documents containing our selected term can never be larger than the total number of documents in our sample. To summarize: idf is a positive quantity depending on a single term and it is computed over a sample of documents. The smaller the idf, the more present is our term in the sample of documents.\nNow that we defined and got familiar with idf, it’s time to see how we use it in the context of text mining. Our goal is still to measure how important a word is in a document or in a collection (or corpus) of documents. We briefly mentioned that the term frequency (tf) alone is a misleading indicator of the importance of a word as most frequent words are often meaningless (for this reason we filtered out stop words). This time we will combine tf and idf into a single quantity called (surprisingly!) tf-idf. When we combine these two quantities, idf will work as a weight for the term frequency, adjusting its magnitude according to the frequency of the term in the entire corpus. It is important to keep in mind that the definition of the tf-idf, and all the statistics related to it, is a heuristic process, i.e. it is a methodology that has been proved to be useful in text mining simply “by trying” and that it has not any specific theoretical motivations behind it. For our practical purpose, this simply means that any conclusion drawn from tf-idf analysis has to been taken with care.\n\nissue_words <- data_df %>%\n  unnest_tokens(word, content) %>%\n  count(issue, word, sort = TRUE)\n\nissue_words <- na.omit(issue_words)\n\ntotal_words <- issue_words %>% \n  group_by(issue) %>% \n  summarize(total = sum(n))\n\nissue_total_words <- left_join(issue_words, total_words) %>% \n  arrange(desc(issue))\n\nJoining with `by = join_by(issue)`\n\n\n\n\n\n\n\nHow does it work?\n\n\n\n\n\n\nWe first tokenize our raw data using unnest_tokens(). Note that our starting data is not anymore the cleaned up data of the previous sessions, but the “original” raw data stored in the DataFrame data_df. We tokenize the column content of our DataFrame by word. We then use the function count() to count the number of words per issue, assigning the result to the DataFrame issue_words.\n\n\nWe remove any possible NA from our DataFrame applying the function na.omit();\n\n\nIn a second R statement, we apply the function group_by() to the just created DataFrame issue_words. The function group_by() splits the DataFrame into groups of identical values in a specified column (in our case issue) so that, from now on, any other operation in the pipeline will be performed on groups of words belonging to the same issues. The function summarise() returns a new DataFrame (total_words) with one row for each group (in our case, one row per issue). Specifying as an argument total = sum(n), we tell R to add a new column on the output DataFrame. This column will be named total and it will contain the sum of all the values of the column n in the issue_words DataFrame per group;\n\n\nFinally, we join (merge) the two previously created DataFrames using the function left_join(). As the join is a left join, we mantain the structure of the first argument of the function, the left DataFrame issue_words, so one row per word and columns specifying the issue where the word was found and the number of times that word appears in that issue. After the joining operation, the output DataFrame issue_total_words will have an additional column named total containing the total number of words per issue. As same issues have same number of total words, there will be repeated values in the total columns, but we will deal with those later.\n\n\n\n\nIn the previous block of code we computed and stored in two DataFrames the frequency of occurrence of each word and the total number of words per issue. For computation and visualisation convenience, we then stored the word count per issue and total number of words per issue in a single DataFrame. Now it’s time to plot the term frequency per issue.\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nHow does it work?\n\n\n\n\n\n\nWe select the issue_total_words DataFrame and using the function filter() we select only those rows (words) that have a value larger than 10000 in the total column, i.e. whose belonging issues have a total number of words larger than 10000. We then apply the function distinct() to obtaing an output DataFrame (unique_issues) containing only distinct values of the column issue, i.e. one issue oer row;\n\n\nUsing the function slice(), we select the first 6 rows (1:6) of the DataFrame unique_issues and we store them in the DataFrame first_6_unique_issues;\n\n\nWe use the function semi_join() to join the DataFrames issue_total_words and first_6_unique_issues according to the column issue. In this way the resulting DataFrame will have the same structure of issue_total_words but will contain only rows with issues corresponding to the ones listed in first_6_unique_issues. For visualization purposes, we then apply the function mutate() to replace the issue column (containing numeric values) with a column having exactly the same name and values, but in a string (character) format. This will help the plot function to label our plots properly;\n\n\nWe feed the previous result to the function ggplot() for plotting. aes() is a function that stands for “aesthetics” and it is used to specify what to plot. In our case we want to plot the terms frequency, i.e. n/total. The color of the generated plots will vary according to the issue column, this is specified with fill=issue. We also indicate that we want to plot a histogram without legend (geom_histogram(show.legend = FALSE)). We also specify the boundary of the x axis with xlim() and that we want plots to be distributed in a grid. To do that we use the function facet_wrap() that will plot several plots according to the column issue in a grid of plots with 2 columns (ncols=2)\n\n\n\n\nLooking at the histograms of the six longest issues in our sample, it is quite evident that the term frequency follows a very similar distribution in almost all the issues. This is because the most frequent terms in all the issues are meaningless stop words and the frequency of stop words does NOT depend on the topic or the particular issue.\nNow that we had a look at the term frequency per issue, let’s compute the tf-idf and let’s compare these two indicators. Once again, we do not have to do that manually as the package tydytext provides us a specific function for this purpose:\n\nissue_tf_idf <- issue_words %>%\n  bind_tf_idf(word, issue, n)\n\nissue_tf_idf %>%\n  arrange(desc(tf))\n\n# A tibble: 574,385 × 6\n   issue word      n    tf   idf tf_idf\n   <int> <chr> <int> <dbl> <dbl>  <dbl>\n 1 68302 the       8 0.170     0      0\n 2 52204 the      31 0.150     0      0\n 3 53256 the      28 0.146     0      0\n 4 53191 the      58 0.146     0      0\n 5 53078 the      27 0.141     0      0\n 6 57761 the      18 0.132     0      0\n 7 53284 the      58 0.130     0      0\n 8 53077 the      26 0.13      0      0\n 9 61094 the      49 0.130     0      0\n10 53175 the      76 0.130     0      0\n# ℹ 574,375 more rows\n\n\n\n\n\n\n\nHow does it work?\n\n\n\n\n\n\nThe function bind_tf_idf() computes term frequency, idf, and multiples them together to obtain tf-idf. Its arguments are the name of the column containing the token (in this dase word), the column containing the unique identifier of the document in our sample (issue) and the column with the number of times the token occurs in the text (n). It then stores all these outputs in a DataFrame (issue_tf_idf) per word;\n\n\nWe sort the output result, the DataFrame issue_tf_idf, by descending values of term frequency with the function arrange().\n\n\n\n\nLooking at the first rows of the output DataFrame, it is clear that the most frequent word is “the”, a stop word. However, when we look at the corresponding tf-idf, its value is zero. This is because its idf is zero and it is an indicator that, indeed, “the” is a common word and I may be important, but it is too much common and that, perhaps, it should be excluded from our analysis.\n\nissue_tf_idf %>%\n  arrange(desc(tf_idf))\n\n# A tibble: 574,385 × 6\n   issue word          n     tf   idf tf_idf\n   <int> <chr>     <int>  <dbl> <dbl>  <dbl>\n 1 68732 cod          21 0.0463  7.17  0.332\n 2 68277 bosnian       2 0.0435  7.17  0.312\n 3 68405 code          2 0.0426  5.38  0.229\n 4 68873 croatia       4 0.0317  7.17  0.228\n 5 68873 rehn          4 0.0317  7.17  0.228\n 6 55541 merlot        3 0.0283  7.17  0.203\n 7 68578 flag          2 0.0455  4.40  0.200\n 8 68890 ceausescu     2 0.0278  7.17  0.199\n 9 68277 agents        2 0.0435  4.53  0.197\n10 68302 wording       2 0.0426  4.61  0.196\n# ℹ 574,375 more rows\n\n\nArranging the tf-idf in descending order, we see in the first rows of our DataFrame rare words, but still not so rare. Rare words may indicate particularly significant events related to “European Union” as these words do not often occure in the Times news corpora. As we did for the term frequency, let’s visualise the tf-idf for the 6 longest issues in our data set:\n\nissue_tf_idf %>%\n  semi_join(first_6_unique_issues, by=\"issue\") %>%\n  group_by(issue) %>%\n  slice_max(tf_idf, n = 10) %>%\n  ungroup() %>%\n  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = issue)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~issue, scales=\"free\",ncol = 2) +\n  labs(x = \"tf-idf\", y = NULL) \n\n\n\n\nLooking at the plots, five of the issues have still a word in common, while one of them (57791) stands out. This already tell us that issue 57791 could deserve particular attention.\n\n\nRelationships Between Words\nIf we look at what we did so far to extract meaningful information from text, our methods look a quite over-simplification compared to the complexity of the human language: we basically counted single words computing their frequency over different samples (single text or entire corpus) and assigned them arbitrary tags to quantify which emotion they were more representative of.\nCounting single words is a well proven text mining technique, but even more interesting is studying the relation between two or more words, i.e. which words tend to follow others immediately or tend to co-occur withing the same document.\nIn this session we will explore tidytext methods that focus on analysing and visualising the relationships between two or more words.\n\n\nTokenization by n-gram\nWe learned that the first step in text mining is tokenization, where the token can be a single character, word, a sentence, a paragraph, etc. In studying the relation between words we will start with the easiest case: relation between two words. In this case, our tokens will be groups of two consecutive words. In order to extract such groups and to render them into a DataFrame, we will use the same tidytext function that we used for tokenizing text in single words: unnest_tokens(). The default token for unnest_tokens() is single words, so that this time we need to specify that we need groups of two words. To do that, we specify the parameters token and n equal to ngrams and 2 respectively.\nAn ngram is just a contiguous sequence of items. How many items? n, so that if n=1 we have only one word, if n=2 we will have groups of two words, n=3 three words, and so on. More in general, ngrams items can be any kind of token (a single character, a word, a sentence, etc), but in this session, for simplicity, we will assume that the n-grams items are just words. If, for example, we consider the sentence “I wish I could be at Bahamas sipping a Pinacolada”, the first four word n-grams will be: - 1-gram (unigram): (“I”,“wish”,“I”,“could”,“be”,“at”,“Bahamas”,“sipping”,“a”,“Pinacolada”), the one we used in the previous paragraphs; - 2-gram (bigram): (“I wish”,“wish I”,“I could”,“could be”,“be at”,“at Bahamas”,“Bahamas sipping”,“sipping a”,“a Pinacolada”); - 3-gram (trigram): (“I wish I”,“wish I could”,“I could be”,“could be at”,“be at Bahamas”,“Bahamas sipping a”,“sipping a Pinacolada”); - 4-gram: (“I wish I could”,“wish I could be”,“I could be at”,“could be at Bahamas”,“be at Bahamas sipping”,“at Bahamas sipping a”, “Bahamas sipping a Pinacolada”).\n\ntidy_content_rel <- data_df %>% unnest_tokens(bigram, content, token=\"ngrams\", n=2)\n\n\n\n\n\n\nHow does it work?\n\n\n\n\n\n\nThe previous line of code takes our raw data, data_df, and it injects it in the unnest_tokens() function. The function will look at the content column of the DataFrame and will split the content into tokens of 2 consecutive words stored in the column bigram of a new created DataFrame called tidy_content_rel.;\n\n\n\n\nLet’s visualise how many times the couple of words in the bigram column occur in the entire corpus:\n\ntidy_content_rel %>%\n  count(bigram, sort = TRUE) %>%\n  filter(n > 2000) %>%\n  mutate(bigram = reorder(bigram, n)) %>%\n  ggplot(aes(n, bigram)) +\n  geom_col() +\n  labs(y = NULL)\n\n\n\n\nThe previous plot is very similar to our very first plot showing the word count in the entire corpus. The two plots are indeed identical, the only difference is that in that previous case the counted tokens were single words while now they are biagrams, i.e. groups of two words. Another thing we can notice is that, once again, the most frequent tokens are stop words. Let’s get rid of them:\n\nbigrams_separated <- tidy_content_rel %>%\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \")\n\nbigrams_filtered <- bigrams_separated %>%\n  filter(!word1 %in% stop_words$word) %>%\n  filter(!word2 %in% stop_words$word)\n\ntidy_content_rel_clean <- bigrams_filtered %>%\n  unite(bigram, word1, word2, sep = \" \")\n\n\n\n\n\n\nHow does it work?\n\n\n\n\n\n\nOur biagram is stored in a single string (character) in the column biagram of our DataFrame tidy_content_rel. In the first instruction of the previous block of code, we first split the column bigram into two different columns. To do that, we use the function separate() specifying as arguments the column to split (bigram), the names of the two new columns (word1 and word2), and the character that separates values in the bigram column (in our case, an empty space). We store the result into the DataFrame bigrams_separated;\n\n\nStarting from bigrams_separated, we filter the content of the new two columns so that none of their content is a stop word. We already have a list of stop words, this is stored in the word column of the stop_words DataFrame, so what we need to do is to tell R to keep only those words that are NOT in that column. We do that with the instruction !word1/2 %in% stop_words$word;\n\n\nRight after tokenization we had our biagram stored in a single column, so as a final step we use the function unite() to merge the content of the now filtered columns word1 and word2 back into the column bigram, separated by a space. We save this result into the DataFrame tidy_content_rel_clean.\n\n\n\n\nAs we did several times before, let’s visualise the counts of our biagram:\n\ntidy_content_rel_clean %>%\n  count(bigram, sort = TRUE) %>%\n  filter(n > 500) %>%\n  mutate(bigram = reorder(bigram, n)) %>%\n  ggplot(aes(n, bigram)) +\n  geom_col() +\n  labs(y = NULL)\n\n\n\n\nWe immediately notice that the stop words are gone and that the most frequent biagrams in our corpus are “total vote” and “lab majority”.\nSometimes, it is useful to keep our bigrams in two distinct columns, one per word. This format could be very handy when we need to apply filtering criteria to only one of the words of the biagram. For example, we might be interested in the most common biagrams where the second word is “vote”. We already obtained this format when computing the DataFrame bigrams_separated:\n\nbigrams_filtered %>% \n  filter(word2 == \"vote\") %>%\n  count(issue, word1, sort=TRUE)\n\n# A tibble: 709 × 3\n   issue word1      n\n   <int> <chr>  <int>\n 1 57897 total   1258\n 2 59216 total   1222\n 3 59028 total    916\n 4 61556 total    602\n 5 59216 rotal     15\n 6 59028 rotal     10\n 7 61556 6          5\n 8 61556 rotal      5\n 9 53127 french     4\n10 61556 96         4\n# ℹ 699 more rows\n\n\nIn this case we obtain the count of all the words preceding the word vote by issue. “total” is the most frequent.\nIn the previous paragraph we mentioned that tf-idf is a function of the selected term. More in general, we can compute tf-idf for the seleced token, whatever it is a single character, word, biagram, n-gram, sentence, etc. To compute the tf-idf for our biagram we do not need to introduce any new tool, but we will simply use the function `bind_tf_idf()’ in a similar way as we did for single words:\n\nbigram_tf_idf <- tidy_content_rel_clean %>%\n  count(issue, bigram) %>%\n  bind_tf_idf(bigram, issue, n) %>%\n  arrange(tf_idf)\n\nbigram_tf_idf\n\n# A tibble: 328,843 × 6\n   issue bigram             n        tf    idf      tf_idf\n   <int> <chr>          <int>     <dbl>  <dbl>       <dbl>\n 1 56145 european union     1 0.0000384 0.0147 0.000000564\n 2 59216 european union     3 0.0000650 0.0147 0.000000954\n 3 56596 european union     2 0.0000686 0.0147 0.00000101 \n 4 61556 european union     6 0.000140  0.0147 0.00000205 \n 5 57897 european union     7 0.000169  0.0147 0.00000248 \n 6 59028 european union     7 0.000179  0.0147 0.00000263 \n 7 56120 european union     1 0.000584  0.0147 0.00000857 \n 8 54346 european union     1 0.000654  0.0147 0.00000959 \n 9 57791 european union     2 0.000893  0.0147 0.0000131  \n10 55250 european union     1 0.000972  0.0147 0.0000143  \n# ℹ 328,833 more rows\n\n\n\n\n\n\n\nHow does it work?\n\n\n\n\n\n\nIn order to compute idf and tf-idf, the function bind_tf_idf() needs our DataFrame and the name of the columns specifying tokens, document identifiers (issues), and the number of times tokens occur in each issue. Our DataFrame tidy_content_rel_clean does not contain this last quantity. To compute this, we apply the function count() specifying to count the number of identical biagrams by issue. This will return a DataFrame with three columns: biagram, issue, and n (the number of times that a biagram occurs in each issue). This result is used as input for bind_tf_idf() and the final result is arranged according to descending td_idf using arrange(tf_idf)."
  },
  {
    "objectID": "text-mining-homework.html#required-homework",
    "href": "text-mining-homework.html#required-homework",
    "title": "Homework",
    "section": "Required Homework",
    "text": "Required Homework\nAs homework, you are required to:\n1. Complete the Text Mining exercises conducted during the session.\nThis is if you didn’t already finish it during the meeting.\nNote that you will be required to submit the completed exercises as text_mining_with_R_notebook_yourname.Rmd, as part of the grading of the course.\n2.a. Finalize your research question for the final assignment. 2.b. Draft a plan for data analysis in a simple text or Word document to answer the question.\nThis plan would contain:\n\nIntroduction of the research question.\nA query that will be run in I-Analyzer to obtain data relevant to your research question.\nDescription of the text mining techniques that you want to apply to your dataset to answer your research question.\n\nThis draft is meant to help you get thinking about how you can approach your final assignment. You can consult with the instructors if you want to further reflect on your research question and data analysis plan.\n2. Obtain data from I-Analyzer relevant to your research question.\n\nYou can effectively make a start on your final assignment and go ahead with downloading a dataset from I-Analyzer (preferably the Times corpus).\nExplain how you obtained the data: i.e. specify the corpus, the search parameters, the export options you selected, the file (and format) you ended up with. Essentially, another person should be able to obtain the same dataset if they follow your description.\nYour description of the process will become a part of the Methods section of your final assignment."
  },
  {
    "objectID": "text-mining-homework.html#bonus-homework",
    "href": "text-mining-homework.html#bonus-homework",
    "title": "Homework",
    "section": "Bonus Homework",
    "text": "Bonus Homework\nWe have some bonus homework if you’r feeling extra motivated. The emojis indicate the difficulty level of the exercise and how delighted the instructors would be if your tried them out.\n\n[Coding] Repeat the sentiment analysis performed in class for different emotions. Check in the lexicon which emotions are listed, pick at least three, perform the analysis as in class, and describe your result 😃😃;\n[Reading/Coding] Choose one of the case studies in the official “Text mining with R” textbook and run the analysis step by step 😃.\n[Searching/Reading] In class lexicons where mentioned. Make a search on internet, try to find at least other 3 alternative lexicons compared to the one we used in class. Make a file listing what these lexicons have in common, their differences, the method used to make them, and your opinion about their reliability 😃😃;\n[Coding] Using ngrams of 4 words, make a DataFrame selecting ngrams where the first three words are “European Union is”. Count how many times different ngrams of this kind occur and display the result both in a table (simply visualizing the DataFrame) and making a plot as a function of the fourth word 😃😃😃;"
  },
  {
    "objectID": "text-mining-homework.html#ask-for-help",
    "href": "text-mining-homework.html#ask-for-help",
    "title": "Homework",
    "section": "Ask For Help!",
    "text": "Ask For Help!\nIf you get stuck, don’t hesitate to reach out to the instructors during the Walk-In Hours of Research Data Management Support. The Walk-In Hours take place every Monday from 15:00 to 17:00 at the University Library in the Science Park. However, one instructor can be available at the University Library in the city center (in the seating area near the Digital Humanities Lab) and you are welcome free to request a meeting online (via MS Teams) during these hours as well.\nYou can also contact the course coordinator, Neha Moopen, by email at n.moopen@uu.nl\nDon’t forget the preparation for the next session!`"
  },
  {
    "objectID": "reproducible-reports-preparation.html#installation-setup",
    "href": "reproducible-reports-preparation.html#installation-setup",
    "title": "Preparation",
    "section": "Installation & Setup",
    "text": "Installation & Setup\nIn addition to programs and packages we’ve installed previously, we will need to install and setup the following:\n\nQuarto & usethis\n\nQuarto, which is a scientific and technical publishing system. This is a tool you will install on your computer and RStudio will be able to access it.\n\nYou can test if Quarto was installed correctly be (re)opening or (re)starting RStudio and clicking through File -> New File and checking if you the options for Quarto-related files like Quarto Document, Quarto Presentation etc.\n\nYou will also need the usethis package in R:\n\n\ninstall.packages(\"usethis\")\n\nlibrary(usethis)\n\n\n\nZotero\n\nZotero, which is an open-source reference management tool.\nZotero Connector, which is a browser plugin that allows you to add references to Zotero with one click.\nBetter BibTex for Zotero, which is an add-on to Zotero that makes it easy to format your bibliography in BibTex format. The installation instructions are relatively clear, but in case it isn’t: the add-on has to be downloaded from GitHub as a .xpi file (i.e. the latest release) but the .xpi file needs to be installed/opened via Zotero itself.\nCreate a Zotero account via their official website.\nSync your local Zotero installation with your online account by using the Sync option in Zotero’s Preferences window. This requires logging in with the account you just created online.\nCreate a folder in your Zotero library - for example, text-mining-literature-review. Add the literature you reviewed in the previous sessions to this folder, you should end up with 4 bibliography items. You could add these files manually or using the Zotero Connector plugin (the latter might be slightly better).\n\n\n\nGitHub\nCreate an account on GitHub. If time allows, we will use GitHub as platform to publish our reproducible report online."
  },
  {
    "objectID": "final-assignment.html#portfolio",
    "href": "final-assignment.html#portfolio",
    "title": "Final Assignment",
    "section": "Portfolio",
    "text": "Portfolio\nThe portfolio contains all the work that you completed during the course, including the various exercises as well as the assignments. When you unzip the folder, you will already see the structure you want to use:\nLEG-SA-11_2023_surname/\n├── 01-i-analyzer/\n├── 02-base-r/\n├── 03-literature-review/\n├── 04-text-mining/\n├── 05-final-assignment/\n└── 06-reflection-assignment/\nMove your completed exercises into the relevant folder, here is a checklist:\n\n2x I-Analyzer datasets (the export made during class + your homework, including the search parameters)\nBase R exercises (completed .Rmd file)\nSlides of literature review\nText Mining in R exercises (completed .Rmd file)\n\nFinally, make sure you rename the folder correctly."
  },
  {
    "objectID": "final-assignment.html#final-assignment",
    "href": "final-assignment.html#final-assignment",
    "title": "Final Assignment",
    "section": "Final Assignment",
    "text": "Final Assignment\nThis folder contains a template project for the final assignment of the course. Follow the instructions here step-by-step and build up your project along the way.\n\nProject Structure\nThe project is structured in the following way:\n05-final-assignment\n├───05-final-assignment.Rproj\n├───data\n├───docs\n├───lexicons\n    ├─── NRC_lexicon.txt  \n├───R\n    ├─── yourstudentnumber-final-assignment.Rmd\n└───README.txt\n1. R Project File\nAlways use the R Project file (05-final-assignment.Rproj) to open your project. This will automatically set the working directory, which is needed to work with some of the template code we will provide.\n2. Data\nPlace the dataset you export from I-Analyzer in the data folder. Make sure to (re)name the dataset as data.csv (note that csv is the file extension) because the name assigned by I-Analyzer to the export can be very long and messy.\n3. Docs\nUse the docs folder to place any supplementary materials such as notes etc.\n4. Lexicons\nThis folder already contains the NRC Lexicon (NRC_lexicon.txt) that you will use in your text-mining analyses.\n5. R\nThis folder contains the template R Markdown file for you to work on your assignment (yourstudentnumber_final-assignment.Rmd). Instructions on how to work with this template continue below in a separate section. Eventually, you will render this R Markdown file to HTML format."
  },
  {
    "objectID": "final-assignment.html#assignment-template",
    "href": "final-assignment.html#assignment-template",
    "title": "Final Assignment",
    "section": "Assignment Template",
    "text": "Assignment Template\nWe assume that you will start working with template after you have exported your dataset from I-Analyzer and placed it in the data folder. The remainder of this section walks you through on how to work with this template\n\nYAML\nIn the YAML (Yet Another Markup Language) section, which are the lines at the very beginning of the source document between three —):\nIn the YAML session (the lines at the very beginning of the source document between three \\(-\\)):\n\nReplace “My_assigment” with the title of your report, based on your research question/query;\nUse your name and surname as author;\nChange the date to the date of submission of your assignment.\n\n\n\nMarkdown Format\nYou will write regular text using the Markdown format in your file. You can refer to this Markdown Cheatsheet. Moreover, you use the Visual Editor in RStudio to preview Markdown content as you go.\n\n\nIntroduction\nHere you will write a small introduction about your research question/query of maximum 400 words. Every introduction usually follows a funnel structure, from general to particular:\n\nIntroduce the topic (in general);\nThen focus on your specific research question/query;\nExplain how and why exploring your research question/query would be interesting;\nExplain (in general) how the analysis you are going to perfrom can answer your research question/query.\n\n\n\nData\nHere you will describe your data, including how you obtained it from I-Analyzer. Be sure to include the following information:\n\nThe corpus of data and eventual references associated with it;\nEvery single field you used in your I-Analyzer query;\nAny other information relative to the corpus and the analysis (data timeline etc.).\n\n\n\nAnalysis & Results\nThis is the only section of your assignment template that will contain code. Before performing any analyses, write a few lines on the text-mining techniques you are going to apply and what kind of results you expect to obtain from the analysis. In other words, provide a very brief overview of your data analysis plan in about 100 words at maximum.\nAs you move through this section, before every block of code, provide a couple of lines describing what the specific code block is intended to do.\nIn some cases, we have provided some template code blocks, which you will have to adjust to suit your data - much like the exercises in class. As you go through the steps of analyzing your data, you will have to build the code blocks more independently (though you can always refer to the earlier course materials).\nYou can add code chunks as required by clicking the Add Chunk button. In case you need a reference, here is a link: https://rmarkdown.rstudio.com/lesson-3.html\nAfter every code block, provide a couple of lines to describe the results of the operation. In some cases, it may be as simple as stating the number of values that were dropped after filtering NAs. In other cases, there may be more interesting output in the form of a table or plot. In this case, describe the results and interpret the findings in the context of your research question/query. Leave any conclusions for the next section.\n\n\nConclusion\nHere you will summarize your analyses as a whole. What are your results and findings? What is your main conclusion (the short answer to your research question/query, if you found any. What are the possibilities of future analyses or studies to better explore the question/query. This conclusion can be a maximum of 300 words.\n\n\nReferences\nAt minimum, you should include the following references:\n\nThe corpus your referred to in I-Analyzer\nThe textbook that was used for this course (Text-Mining with I-Analyzer & R).\nThe Text Mining with R: A Tidy Approach book.\n\nAdd the references in BibTex format to the references.bib file. You can then insert them in your text using like @moopen2023 for an unbracketed reference or (@moopen2023) for a bracketed reference.\n\n\nRender to HTML\nWhen you are completed with your assignment and the code chunks are running smoothly, you can click the ‘Knit’ button in the RStudio menu to render/convert the R Markdown file to HTML format."
  },
  {
    "objectID": "final-assignment.html#reflection-assignment",
    "href": "final-assignment.html#reflection-assignment",
    "title": "Final Assignment",
    "section": "Reflection Assignment",
    "text": "Reflection Assignment\nThe reflection assignment is available as a Word document template. This should be completed as per the instructions in the document and the file should be renamed with your student number and name."
  }
]