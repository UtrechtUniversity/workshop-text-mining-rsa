group_by(issue) %>%
slice_max(tf_idf, n = 15) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = issue)) +
geom_col(show.legend = FALSE) +
facet_wrap(~issue, scales="free",ncol = 2) +
labs(x = "tf-idf", y = NULL)
library(forcats)
issue_tf_idf %>%
filter(total > 50000) %>%
group_by(issue) %>%
slice_max(tf_idf, n = 10) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = issue)) +
geom_col(show.legend = FALSE) +
facet_wrap(~issue, scales="free",ncol = 2) +
labs(x = "tf-idf", y = NULL)
bigram_tf_idf <- bigrams_united %>%
count(issue, bigram) %>%
bind_tf_idf(bigram, issue, n) %>%
arrange(desc(tf_idf))
library(readr)
data_file_name <- '../data/times_ocr=80_100&date=1945-01-01_2010-12-31&query=_european_union_&category=News.csv'
data_df <- read_delim(data_file_name, delim = ";", escape_double = FALSE, col_types = cols(`date-pub` = col_date(format = "%B %d, %Y")), trim_ws = TRUE)
print(colnames(data_df))
library(tidyverse)
library(tidytext)
tidy_content <- data_df %>% unnest_tokens(word, content)
data(stop_words)
tidy_clean_content <- tidy_content %>% anti_join(stop_words)
bigram_tf_idf <- bigrams_united %>%
count(issue, bigram) %>%
bind_tf_idf(bigram, issue, n) %>%
arrange(desc(tf_idf))
bigram_tf_idf
bigram_tf_idf <- bigrams_united %>%
count(issue, bigram) %>%
bind_tf_idf(bigram, issue, n) %>%
arrange(tf_idf)
bigram_tf_idf
bigram_tf_idf <- bigrams_united %>%
count(issue, bigram) %>%
bind_tf_idf(bigram, issue, n) %>%
arrange(idf)
bigram_tf_idf
library(forcats)
bigrams_tf_idf %>%
filter(total > 50000) %>%
group_by(issue) %>%
slice_max(tf_idf, n = 10) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(biagram, tf_idf), fill = issue)) +
geom_col(show.legend = FALSE) +
facet_wrap(~issue, scales="free",ncol = 2) +
labs(x = "tf-idf", y = NULL)
library(forcats)
bigram_tf_idf %>%
filter(total > 50000) %>%
group_by(issue) %>%
slice_max(tf_idf, n = 10) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(biagram, tf_idf), fill = issue)) +
geom_col(show.legend = FALSE) +
facet_wrap(~issue, scales="free",ncol = 2) +
labs(x = "tf-idf", y = NULL)
View(bigram_tf_idf)
library(forcats)
bigram_tf_idf %>%
group_by(issue) %>%
slice_max(tf_idf, n = 10) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(biagram, tf_idf), fill = issue)) +
geom_col(show.legend = FALSE) +
facet_wrap(~issue, scales="free",ncol = 2) +
labs(x = "tf-idf", y = NULL)
library(forcats)
bigram_tf_idf %>%
group_by(issue) %>%
slice_max(tf_idf, n = 10) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(bigram, tf_idf), fill = issue)) +
geom_col(show.legend = FALSE) +
facet_wrap(~issue, scales="free",ncol = 2) +
labs(x = "tf-idf", y = NULL)
library(forcats)
bigram_tf_idf %>%
group_by(issue) %>%
slice_max(tf_idf, n = 10) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(bigram, tf_idf), fill = issue)) +
geom_col(show.legend = FALSE) +
facet_wrap(~issue, scales="free",ncol = 2)
library(forcats)
bigram_tf_idf %>%
group_by(issue) %>%
slice_max(tf_idf, n = 10) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(bigram, tf_idf), fill = issue)) +
geom_col(show.legend = FALSE) +
facet_wrap(~issue, scales="free",ncol = 2) +
labs(x = "tf-idf", y = NULL)
View(issue_tf_idf)
View(issue_tf_idf)
library(forcats)
bigram_tf_idf %>%
arrange(desc(tf_idf)) %>%
group_by(issue) %>%
slice_max(tf_idf, n = 10) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(bigram, tf_idf), fill = issue)) +
geom_col(show.legend = FALSE) +
facet_wrap(~issue, scales="free",ncol = 2) +
labs(x = "tf-idf", y = NULL)
View(issue_words)
View(issue_words)
View(total_words)
View(total_words)
test <- issue_words %>%
group_by(issue)
View(test)
View(test)
View(issue_total_words)
View(issue_total_words)
library(ggplot2)
unique_issues <- issue_total_words %>% filter(total>10000) %>% distinct(issue)
first_6_unique_issues <- unique_issues %>% slice(1:6)
issue_total_words %>% semi_join(first_6_unique_issues, by="issue") %>%
mutate(issue=as.character(issue)) %>%
ggplot(aes(n/total, fill = issue)) +
geom_histogram(show.legend = FALSE) +
xlim(NA, 0.0005) +
facet_wrap(~issue, ncol = 2, scales = "free_y")
View(issue_total_words)
View(issue_total_words)
View(issue_words)
View(issue_words)
issue_tf_idf <- issue_total_words %>%
mutate(issue=as.character(issue)) %>%
bind_tf_idf(word, issue, n)
library(readr)
data_file_name <- '../data/times_ocr=80_100&date=1945-01-01_2010-12-31&query=_european_union_&category=News.csv'
data_df <- read_delim(data_file_name, delim = ";", escape_double = FALSE, col_types = cols(`date-pub` = col_date(format = "%B %d, %Y")), trim_ws = TRUE)
print(colnames(data_df))
library(readr)
data_file_name <- '../data/times_ocr=80_100&date=1945-01-01_2010-12-31&query=_european_union_&category=News.csv'
data_df <- read_delim(data_file_name, delim = ";", escape_double = FALSE, col_types = cols(`date-pub` = col_date(format = "%B %d, %Y")), trim_ws = TRUE)
print(colnames(data_df))
library(tidyverse)
library(tidytext)
tidy_content <- data_df %>% unnest_tokens(word, content)
issue_tf_idf <- issue_total_words %>%
mutate(issue=as.character(issue)) %>%
bind_tf_idf(word, issue, n)
issue_tf_idf %>%
arrange(tf_idf)
test <- issue_total_words %>%
mutate(idf=log(total/(n+1)))
View(test)
View(test)
test <- issue_total_words %>%
mutate(idf=log(total/(n)))
issue_tf_idf <- issue_words %>%
mutate(issue=as.character(issue)) %>%
bind_tf_idf(word, issue, n)
issue_tf_idf %>%
arrange(tf_idf)
issue_tf_idf <- issue_words %>%
bind_tf_idf(word, issue, n)
issue_tf_idf %>%
arrange(tf_idf)
test <- issue_total_words %>%
mutate(idf=log(total/(n)))
test2 <- issue_total_words %>% filter(n>total)
View(test2)
View(test2)
bind_tf_idf2 <- function(tbl, term, document, n) {
term <- quo_name(enquo(term))
document <- quo_name(enquo(document))
n_col <- quo_name(enquo(n))
terms <- as.character(tbl[[term]])
documents <- as.character(tbl[[document]])
n <- tbl[[n_col]]
doc_totals <- tapply(n, documents, sum)
idf <- log(length(doc_totals) / table(terms))
tbl$tf <- n / as.numeric(doc_totals[documents])
tbl$idf <- as.numeric(idf[terms])
tbl$tf_idf <- tbl$tf * tbl$idf
if(any(tbl$idf < 0, na.rm = TRUE)) {
rlang::warn(paste("A value for tf_idf is negative:\n",
"Input should have exactly one row per document-term combination."))
}
tbl
}
issue_tf_idf <- issue_words %>%
bind_tf_idf2(word, issue, n)
issue_tf_idf %>%
arrange(tf_idf)
bind_tf_idf2 <- function(tbl, term, document, n) {
term <- quo_name(enquo(term))
document <- quo_name(enquo(document))
n_col <- quo_name(enquo(n))
terms <- as.character(tbl[[term]])
documents <- as.character(tbl[[document]])
n <- tbl[[n_col]]
doc_totals <- tapply(n, documents, sum)
test3 <- data.frame(documents, terms, n, doct_totals)
idf <- log(length(doc_totals) / table(terms))
tbl$tf <- n / as.numeric(doc_totals[documents])
tbl$idf <- as.numeric(idf[terms])
tbl$tf_idf <- tbl$tf * tbl$idf
if(any(tbl$idf < 0, na.rm = TRUE)) {
rlang::warn(paste("A value for tf_idf is negative:\n",
"Input should have exactly one row per document-term combination."))
}
tbl
}
issue_tf_idf <- issue_words %>%
bind_tf_idf2(word, issue, n)
bind_tf_idf2 <- function(tbl, term, document, n) {
term <- quo_name(enquo(term))
document <- quo_name(enquo(document))
n_col <- quo_name(enquo(n))
terms <- as.character(tbl[[term]])
documents <- as.character(tbl[[document]])
n <- tbl[[n_col]]
doc_totals <- tapply(n, documents, sum)
test3 <- data.frame(documents, terms, n, doc_totals)
idf <- log(length(doc_totals) / table(terms))
tbl$tf <- n / as.numeric(doc_totals[documents])
tbl$idf <- as.numeric(idf[terms])
tbl$tf_idf <- tbl$tf * tbl$idf
if(any(tbl$idf < 0, na.rm = TRUE)) {
rlang::warn(paste("A value for tf_idf is negative:\n",
"Input should have exactly one row per document-term combination."))
}
tbl
}
issue_tf_idf <- issue_words %>%
bind_tf_idf2(word, issue, n)
issue_tf_idf <- issue_words %>%
bind_tf_idf(word, issue, n)
issue_tf_idf %>%
arrange(tf_idf)
term <- quo_name(enquo(word))
document <- quo_name(enquo(issue))
doc_totals <- tapply(issue_words$n, issue_words$issue, sum)
idf <- log(length(doc_totals) / table(issue_word$words))
doc_totals <- tapply(issue_words$n, issue_words$issue, sum)
idf <- log(length(doc_totals) / table(issue_words$words))
doc_totals <- tapply(issue_words$n, issue_words$issue, sum)
idf <- log(length(doc_totals) / table(issue_words$word))
doc_totals <- tapply(issue_words$n, issue_words$issue, sum)
idf <- log(length(doc_totals) / table(issue_words$word))
doc_totals <- tapply(issue_words$n, issue_words$issue, sum)
idf <- log(length(doc_totals) / table(issue_words$word))
idf[$word]
doc_totals <- tapply(issue_words$n, issue_words$issue, sum)
idf <- log(length(doc_totals) / table(issue_words$word))
idf
library(readr)
data_file_name <- '../data/times_ocr=80_100&date=1945-01-01_2010-12-31&query=_european_union_&category=News.csv'
data_df <- read_delim(data_file_name, delim = ";", escape_double = FALSE, col_types = cols(`date-pub` = col_date(format = "%B %d, %Y")), trim_ws = TRUE)
print(colnames(data_df))
View(data_df)
View(data_df)
library(tidyverse)
library(tidytext)
tidy_content <- data_df %>% unnest_tokens(word, content)
View(tidy_content)
library(dplyr)
library(tidytext)
issue_words <- data_df %>%
unnest_tokens(word, content) %>%
count(issue, word, sort = TRUE)
View(issue_words)
View(issue_words)
View(issue_words)
View(issue_words)
total_words <- issue_words %>%
group_by(issue) %>%
summarize(total = sum(n))
issue_total_words <- left_join(issue_words, total_words) %>% arrange(desc(issue))
View(total_words)
View(total_words)
View(issue_total_words)
View(issue_total_words)
issue_tf_idf <- issue_words %>%
bind_tf_idf(word, issue, n)
issue_tf_idf %>%
arrange(tf_idf)
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(tidyverse)
library(tidytext)
library(dplyr)
data_file_name <- '../data/times_ocr=80_100&date=1945-01-01_2010-12-31&query=_european_union_&category=News.csv'
data_df <- read_delim(data_file_name, delim = ";", escape_double = FALSE, col_types = cols(`date-pub` = col_date(format = "%B %d, %Y")), trim_ws = TRUE)
setwd("/Volumes/Samsung_T5/uu_job/workshops/workshop-text-mining-rsa")
library(readr)
library(tidyverse)
library(tidytext)
library(dplyr)
data_file_name <- '../data/times_ocr=80_100&date=1945-01-01_2010-12-31&query=_european_union_&category=News.csv'
data_df <- read_delim(data_file_name, delim = ";", escape_double = FALSE, col_types = cols(`date-pub` = col_date(format = "%B %d, %Y")), trim_ws = TRUE)
setwd("/Volumes/Samsung_T5/uu_job/workshops/workshop-text-mining-rsa/book")
library(readr)
library(tidyverse)
library(tidytext)
library(dplyr)
data_file_name <- '../data/times_ocr=80_100&date=1945-01-01_2010-12-31&query=_european_union_&category=News.csv'
data_df <- read_delim(data_file_name, delim = ";", escape_double = FALSE, col_types = cols(`date-pub` = col_date(format = "%B %d, %Y")), trim_ws = TRUE)
setwd("/Volumes/Samsung_T5/uu_job/workshops/workshop-text-mining-rsa/book")
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(tidyverse)
library(tidytext)
library(dplyr)
data_file_name <- '../data/times_ocr=80_100&date=1945-01-01_2010-12-31&query=_european_union_&category=News.csv'
data_df <- read_delim(data_file_name, delim = ";", escape_double = FALSE, col_types = cols(`date-pub` = col_date(format = "%B %d, %Y")), trim_ws = TRUE)
getwd()
library(readr)
library(tidyverse)
library(tidytext)
library(dplyr)
data_file_name <- '../data/times_ocr=80_100&date=1945-01-01_2010-12-31&query=_european_union_&category=News.csv'
data_df <- read_delim(data_file_name, delim = ";", escape_double = FALSE, col_types = cols(`date-pub` = col_date(format = "%B %d, %Y")), trim_ws = TRUE)
library(readr)
library(tidyverse)
library(tidytext)
library(dplyr)
data_file_name <- '../data/times_ocr=80_100&date=1945-01-01_2010-12-31&query=_european_union_&category=News.csv'
data_df <- read_delim(data_file_name, delim = ";", escape_double = FALSE, col_types = cols(`date-pub` = col_date(format = "%B %d, %Y")), trim_ws = TRUE)
issue_words <- data_df %>%
unnest_tokens(word, content) %>%
count(issue, word, sort = TRUE)
issue_tf_idf <- issue_words %>%
bind_tf_idf(word, issue, n)
issue_tf_idf %>%
arrange(tf_idf)
library(ggplot2)
unique_issues <- issue_total_words %>% filter(total>10000) %>% distinct(issue)
library(tidyverse)
library(readr)
data_file_name <- '../data/times_ocr=80_100&date=1945-01-01_2010-12-31&query=_european_union_&category=News.csv'
data_df <- read_delim(data_file_name, delim = ";", escape_double = FALSE, col_types = cols(`date-pub` = col_date(format = "%B %d, %Y")), trim_ws = TRUE)
print(colnames(data_df))
library(tidytext)
tidy_content <- data_df %>% unnest_tokens(word, content, token="words")
tidy_content
library(tidytext)
tidy_content <- data_df %>% unnest_tokens(word, content, token="words")
tidy_content
nrc_lexicon_df <- read.table("../data/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt", header = FALSE, sep = "\t", stringsAsFactors = FALSE, col.names = c("word", "emotion", "score"))
joy_words <- nrc_lexicon_df  %>%
filter(emotion == "joy" & score == 1)
issue_df <- tidy_clean_content %>%
filter(`date-pub`>='2000-01-01' & `date-pub` < '2010-01-01') %>%
group_by(issue) %>%
reframe(words_per_issue = n(), date= `date-pub`) %>%
unique()
issue_joy_df <- tidy_clean_content %>%
filter(`date-pub`>='2000-01-01' & `date-pub` < '2010-01-01') %>%
inner_join(joy_words) %>%
group_by(issue) %>%
reframe(joy_words_per_issue = n())
issue_tot_df <- merge(issue_df, issue_joy_df, by='issue')
library(dplyr)
library(tidytext)
issue_words <- data_df %>%
unnest_tokens(word, content) %>%
count(issue, word, sort = TRUE)
issue_words <- na.omit(issue_words)
total_words <- issue_words %>%
group_by(issue) %>%
summarize(total = sum(n))
issue_total_words <- left_join(issue_words, total_words) %>% arrange(desc(issue))
library(ggplot2)
unique_issues <- issue_total_words %>% filter(total>10000) %>% distinct(issue)
first_6_unique_issues <- unique_issues %>% slice(1:6)
issue_total_words6 <- issue_total_words %>%
semi_join(first_6_unique_issues, by="issue") %>%
mutate(issue=as.character(issue))
issue_total_words6 %>%
ggplot(aes(n/total, fill = issue)) +
geom_histogram(show.legend = FALSE) +
xlim(NA, 0.0005) +
facet_wrap(~issue, ncol = 2, scales = "free_y")
issue_tf_idf <- issue_words %>%
bind_tf_idf(word, issue, n)
issue_tf_idf %>%
arrange(tf_idf)
issue_tf_idf <- issue_words %>%
bind_tf_idf(word, issue, n)
issue_tf_idf %>%
arrange(tf)
issue_tf_idf <- issue_words %>%
bind_tf_idf(word, issue, n)
issue_tf_idf %>%
arrange(desc(tf))
issue_tf_idf <- issue_words %>%
mutate(issue=as.character(issue)) %>%
bind_tf_idf(word, issue, n)
issue_tf_idf %>%
arrange(desc(tf))
issue_tf_idf <- issue_words %>%
bind_tf_idf(word, issue, n)
issue_tf_idf %>%
arrange(desc(tf))
library(tidytext)
tidy_content <- data_df %>% unnest_tokens(word, content, token="words")
tidy_content
are_there_na <- any(is.na(tidy_content))
if (are_there_na) {
print('WARNING! There are NA in your data!')
} else {
print('Lucky you! I could not find any NA in your data (yet...)!')
}
tidy_content <- na.omit(tidy_content)
are_there_na <- any(is.na(tidy_content))
if (are_there_na) {
print('WARNING! There are NA in your data!')
} else {
print('Lucky you! I could not find any NA in your data (yet...)!')
}
data(stop_words)
tidy_clean_content <- tidy_content %>% anti_join(stop_words)
tidy_clean_content
word_count <- tidy_clean_content %>%
count(word) %>%
filter(n > 2000) %>%
mutate(word = reorder(word, n))
word_count %>%
ggplot(aes(n, word)) +
geom_col() +
labs(y = NULL)
library(ggplot2)
word_count <- tidy_clean_content %>%
count(word) %>%
filter(n > 2000) %>%
mutate(word = reorder(word, n))
word_count %>%
ggplot(aes(n, word)) +
geom_col() +
labs(y = NULL)
library(ggplot2)
word_count <- tidy_clean_content %>%
count(word, sort = TRUE) %>%
filter(n > 2000) %>%
mutate(word = reorder(word, n))
word_count %>%
ggplot(aes(n, word)) +
geom_col() +
labs(y = NULL)
library(ggplot2)
word_count <- tidy_clean_content %>%
count(word) %>%
filter(n > 2000) %>%
mutate(word = reorder(word, n))
word_count %>%
ggplot(aes(n, word)) +
geom_col() +
labs(y = NULL)
word_count
library(tidytext)
tidy_content <- data_df %>% unnest_tokens(word, content, token="words")
tidy_content
are_there_na <- any(is.na(tidy_content))
if (are_there_na) {
print('WARNING! There are NA in your data!')
} else {
print('Lucky you! I could not find any NA in your data (yet...)!')
}
tidy_content <- na.omit(tidy_content)
library(tidytext)
tidy_content <- data_df %>% unnest_tokens(word, content, token="words")
tidy_content
tidy_content <- tidy_content[complete.cases(tidy_content), ]
library(tidytext)
tidy_content <- data_df %>% unnest_tokens(word, content, token="words")
tidy_content
tidy_content <- tidy_content[!is.na(tidy_content$issues), ]
tidy_content <- tidy_content[!is.na(tidy_content$issue), ]
are_there_na <- any(is.na(tidy_content))
if (are_there_na) {
print('WARNING! There are NA in your data!')
} else {
print('Lucky you! I could not find any NA in your data (yet...)!')
}
are_there_na <- any(is.na(tidy_content$issue))
if (are_there_na) {
print('WARNING! There are NA in your data!')
} else {
print('Lucky you! I could not find any NA in your data (yet...)!')
}
are_there_na <- any(is.na(tidy_content$issue))
if (are_there_na) {
print('WARNING! There are NA in your data!')
} else {
print('Lucky you! I could not find any NA in your data (yet...)!')
}
data(stop_words)
tidy_clean_content <- tidy_content %>% anti_join(stop_words)
tidy_clean_content
library(ggplot2)
word_count <- tidy_clean_content %>%
count(word) %>%
filter(n > 2000) %>%
mutate(word = reorder(word, n))
word_count %>%
ggplot(aes(n, word)) +
geom_col() +
labs(y = NULL)
