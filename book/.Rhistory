group_by(issue) %>%
slice_max(tf_idf, n = 15) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = issue)) +
geom_col(show.legend = FALSE) +
facet_wrap(~issue, scales="free_y",ncol = 2) +
labs(x = "tf-idf", y = NULL) +
theme(strip.text.y = element_text(margin = margin(r = 10)))  # Adjust the margin between y-axis tick values
library(forcats)
issue_tf_idf %>%
filter(total > 10000) %>%
group_by(issue) %>%
slice_max(tf_idf, n = 15) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = issue)) +
geom_col(show.legend = FALSE) +
facet_wrap(~issue, scales="free_y",ncol = 2) +
labs(x = "tf-idf", y = NULL) +
theme(strip.text.y = element_text(margin = margin(r = 20)))  # Adjust the margin between y-axis tick values
library(forcats)
issue_tf_idf %>%
filter(total > 10000) %>%
group_by(issue) %>%
slice_max(tf_idf, n = 15) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = issue)) +
geom_col(show.legend = FALSE) +
facet_wrap(~issue, scales="free_y",ncol = 2) +
labs(x = "tf-idf", y = NULL) +
theme(strip.text.y = element_text(margin = margin(r = 100)))  # Adjust the margin between y-axis tick values
library(forcats)
issue_tf_idf %>%
filter(total > 10000) %>%
group_by(issue) %>%
slice_max(tf_idf, n = 15) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = issue)) +
geom_col(show.legend = FALSE) +
facet_wrap(~issue, scales="free_y",ncol = 2) +
labs(x = "tf-idf", y = NULL) +
theme(strip.text.y = element_text(margin = margin(r = 100)))  # Adjust the margin between y-axis tick values
issue_tf_idf %>%
arrange(desc(tf_idf))
library(forcats)
issue_tf_idf %>%
filter(total > 10000) %>%
group_by(issue) %>%
slice_max(tf_idf, n = 15) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = issue)) +
geom_col(show.legend = FALSE) +
facet_wrap(~issue, scales="free_y",ncol = 2) +
labs(x = "tf-idf", y = NULL) +
theme(strip.text.y = element_text(margin = margin(r = 1000)))  # Adjust the margin between y-axis tick values
library(forcats)
issue_tf_idf %>%
filter(total > 10000) %>%
group_by(issue) %>%
slice_max(tf_idf, n = 15) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = issue)) +
geom_col(show.legend = FALSE) +
facet_wrap(~issue, scales="free_y",ncol = 2) +
labs(x = "tf-idf", y = NULL) +
theme(strip.text.y = element_text(size = 8))  # Adjust the margin between y-axis tick values
library(forcats)
issue_tf_idf %>%
filter(total > 10000) %>%
group_by(issue) %>%
slice_max(tf_idf, n = 15) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = issue)) +
geom_col(show.legend = FALSE) +
facet_wrap(~issue, scales="free_y",ncol = 2) +
labs(x = "tf-idf", y = NULL) +
theme(strip.text.y = element_text(size = 20))  # Adjust the margin between y-axis tick values
library(forcats)
issue_tf_idf %>%
filter(total > 10000) %>%
group_by(issue) %>%
slice_max(tf_idf, n = 15) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = issue)) +
geom_col(show.legend = FALSE) +
facet_wrap(~issue, scales="free_y",ncol = 2) +
labs(x = "tf-idf", y = NULL) +
theme(strip.text.y = element_text(size = 50))  # Adjust the margin between y-axis tick values
library(forcats)
issue_tf_idf %>%
filter(total > 10000) %>%
group_by(issue) %>%
slice_max(tf_idf, n = 15) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = issue)) +
geom_col(show.legend = FALSE) +
facet_wrap(~issue, scales="free",ncol = 2) +
labs(x = "tf-idf", y = NULL) +
theme(strip.text.y = element_text(size = 50))  # Adjust the margin between y-axis tick values
library(forcats)
issue_tf_idf %>%
filter(total > 10000) %>%
group_by(issue) %>%
slice_max(tf_idf, n = 15) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = issue)) +
geom_col(show.legend = FALSE) +
facet_wrap(~issue, scales="free",ncol = 2) +
labs(x = "tf-idf", y = NULL) +
theme(strip.text.y = element_text(size = 100))  # Adjust the margin between y-axis tick values
library(forcats)
issue_tf_idf %>%
filter(total > 10000) %>%
group_by(issue) %>%
slice_max(tf_idf, n = 15) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = issue)) +
geom_col(show.legend = FALSE) +
facet_wrap(~issue, scales="free",ncol = 2) +
labs(x = "tf-idf", y = NULL) +
theme(strip.text.y = element_text(size = 1))  # Adjust the margin between y-axis tick values
library(forcats)
issue_tf_idf %>%
filter(total > 10000) %>%
group_by(issue) %>%
slice_max(tf_idf, n = 15) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = issue)) +
geom_col(show.legend = FALSE) +
facet_wrap(~issue, scales="free",ncol = 2) +
labs(x = "tf-idf", y = NULL)
library(forcats)
issue_tf_idf %>%
filter(total > 10000) %>%
group_by(issue) %>%
slice_max(tf_idf, n = 15) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = issue)) +
geom_col(show.legend = FALSE) +
facet_wrap(~issue, scales="y_free",ncol = 2) +
labs(x = "tf-idf", y = NULL)
library(forcats)
issue_tf_idf %>%
filter(total > 10000) %>%
group_by(issue) %>%
slice_max(tf_idf, n = 15) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = issue)) +
geom_col(show.legend = FALSE) +
facet_wrap(~issue, scales="free_y",ncol = 2) +
labs(x = "tf-idf", y = NULL)
library(forcats)
issue_tf_idf %>%
filter(total > 100000) %>%
group_by(issue) %>%
slice_max(tf_idf, n = 15) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = issue)) +
geom_col(show.legend = FALSE) +
facet_wrap(~issue, scales="free_y",ncol = 2) +
labs(x = "tf-idf", y = NULL)
library(forcats)
issue_tf_idf %>%
filter(total > 50000) %>%
group_by(issue) %>%
slice_max(tf_idf, n = 15) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = issue)) +
geom_col(show.legend = FALSE) +
facet_wrap(~issue, scales="free_y",ncol = 2) +
labs(x = "tf-idf", y = NULL)
library(forcats)
issue_tf_idf %>%
filter(total > 50000) %>%
group_by(issue) %>%
slice_max(tf_idf, n = 15) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = issue)) +
geom_col(show.legend = FALSE) +
facet_wrap(~issue, scales="free",ncol = 2) +
labs(x = "tf-idf", y = NULL)
library(forcats)
issue_tf_idf %>%
filter(total > 50000) %>%
group_by(issue) %>%
slice_max(tf_idf, n = 10) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = issue)) +
geom_col(show.legend = FALSE) +
facet_wrap(~issue, scales="free",ncol = 2) +
labs(x = "tf-idf", y = NULL)
bigram_tf_idf <- bigrams_united %>%
count(issue, bigram) %>%
bind_tf_idf(bigram, issue, n) %>%
arrange(desc(tf_idf))
library(readr)
data_file_name <- '../data/times_ocr=80_100&date=1945-01-01_2010-12-31&query=_european_union_&category=News.csv'
data_df <- read_delim(data_file_name, delim = ";", escape_double = FALSE, col_types = cols(`date-pub` = col_date(format = "%B %d, %Y")), trim_ws = TRUE)
print(colnames(data_df))
library(tidyverse)
library(tidytext)
tidy_content <- data_df %>% unnest_tokens(word, content)
data(stop_words)
tidy_clean_content <- tidy_content %>% anti_join(stop_words)
bigram_tf_idf <- bigrams_united %>%
count(issue, bigram) %>%
bind_tf_idf(bigram, issue, n) %>%
arrange(desc(tf_idf))
bigram_tf_idf
bigram_tf_idf <- bigrams_united %>%
count(issue, bigram) %>%
bind_tf_idf(bigram, issue, n) %>%
arrange(tf_idf)
bigram_tf_idf
bigram_tf_idf <- bigrams_united %>%
count(issue, bigram) %>%
bind_tf_idf(bigram, issue, n) %>%
arrange(idf)
bigram_tf_idf
library(forcats)
bigrams_tf_idf %>%
filter(total > 50000) %>%
group_by(issue) %>%
slice_max(tf_idf, n = 10) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(biagram, tf_idf), fill = issue)) +
geom_col(show.legend = FALSE) +
facet_wrap(~issue, scales="free",ncol = 2) +
labs(x = "tf-idf", y = NULL)
library(forcats)
bigram_tf_idf %>%
filter(total > 50000) %>%
group_by(issue) %>%
slice_max(tf_idf, n = 10) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(biagram, tf_idf), fill = issue)) +
geom_col(show.legend = FALSE) +
facet_wrap(~issue, scales="free",ncol = 2) +
labs(x = "tf-idf", y = NULL)
View(bigram_tf_idf)
library(forcats)
bigram_tf_idf %>%
group_by(issue) %>%
slice_max(tf_idf, n = 10) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(biagram, tf_idf), fill = issue)) +
geom_col(show.legend = FALSE) +
facet_wrap(~issue, scales="free",ncol = 2) +
labs(x = "tf-idf", y = NULL)
library(forcats)
bigram_tf_idf %>%
group_by(issue) %>%
slice_max(tf_idf, n = 10) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(bigram, tf_idf), fill = issue)) +
geom_col(show.legend = FALSE) +
facet_wrap(~issue, scales="free",ncol = 2) +
labs(x = "tf-idf", y = NULL)
library(forcats)
bigram_tf_idf %>%
group_by(issue) %>%
slice_max(tf_idf, n = 10) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(bigram, tf_idf), fill = issue)) +
geom_col(show.legend = FALSE) +
facet_wrap(~issue, scales="free",ncol = 2)
library(forcats)
bigram_tf_idf %>%
group_by(issue) %>%
slice_max(tf_idf, n = 10) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(bigram, tf_idf), fill = issue)) +
geom_col(show.legend = FALSE) +
facet_wrap(~issue, scales="free",ncol = 2) +
labs(x = "tf-idf", y = NULL)
View(issue_tf_idf)
View(issue_tf_idf)
library(forcats)
bigram_tf_idf %>%
arrange(desc(tf_idf)) %>%
group_by(issue) %>%
slice_max(tf_idf, n = 10) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(bigram, tf_idf), fill = issue)) +
geom_col(show.legend = FALSE) +
facet_wrap(~issue, scales="free",ncol = 2) +
labs(x = "tf-idf", y = NULL)
View(issue_words)
View(issue_words)
View(total_words)
View(total_words)
test <- issue_words %>%
group_by(issue)
View(test)
View(test)
View(issue_total_words)
View(issue_total_words)
library(ggplot2)
unique_issues <- issue_total_words %>% filter(total>10000) %>% distinct(issue)
first_6_unique_issues <- unique_issues %>% slice(1:6)
issue_total_words %>% semi_join(first_6_unique_issues, by="issue") %>%
mutate(issue=as.character(issue)) %>%
ggplot(aes(n/total, fill = issue)) +
geom_histogram(show.legend = FALSE) +
xlim(NA, 0.0005) +
facet_wrap(~issue, ncol = 2, scales = "free_y")
View(issue_total_words)
View(issue_total_words)
View(issue_words)
View(issue_words)
issue_tf_idf <- issue_total_words %>%
mutate(issue=as.character(issue)) %>%
bind_tf_idf(word, issue, n)
library(readr)
data_file_name <- '../data/times_ocr=80_100&date=1945-01-01_2010-12-31&query=_european_union_&category=News.csv'
data_df <- read_delim(data_file_name, delim = ";", escape_double = FALSE, col_types = cols(`date-pub` = col_date(format = "%B %d, %Y")), trim_ws = TRUE)
print(colnames(data_df))
library(readr)
data_file_name <- '../data/times_ocr=80_100&date=1945-01-01_2010-12-31&query=_european_union_&category=News.csv'
data_df <- read_delim(data_file_name, delim = ";", escape_double = FALSE, col_types = cols(`date-pub` = col_date(format = "%B %d, %Y")), trim_ws = TRUE)
print(colnames(data_df))
library(tidyverse)
library(tidytext)
tidy_content <- data_df %>% unnest_tokens(word, content)
issue_tf_idf <- issue_total_words %>%
mutate(issue=as.character(issue)) %>%
bind_tf_idf(word, issue, n)
issue_tf_idf %>%
arrange(tf_idf)
test <- issue_total_words %>%
mutate(idf=log(total/(n+1)))
View(test)
View(test)
test <- issue_total_words %>%
mutate(idf=log(total/(n)))
issue_tf_idf <- issue_words %>%
mutate(issue=as.character(issue)) %>%
bind_tf_idf(word, issue, n)
issue_tf_idf %>%
arrange(tf_idf)
issue_tf_idf <- issue_words %>%
bind_tf_idf(word, issue, n)
issue_tf_idf %>%
arrange(tf_idf)
test <- issue_total_words %>%
mutate(idf=log(total/(n)))
test2 <- issue_total_words %>% filter(n>total)
View(test2)
View(test2)
bind_tf_idf2 <- function(tbl, term, document, n) {
term <- quo_name(enquo(term))
document <- quo_name(enquo(document))
n_col <- quo_name(enquo(n))
terms <- as.character(tbl[[term]])
documents <- as.character(tbl[[document]])
n <- tbl[[n_col]]
doc_totals <- tapply(n, documents, sum)
idf <- log(length(doc_totals) / table(terms))
tbl$tf <- n / as.numeric(doc_totals[documents])
tbl$idf <- as.numeric(idf[terms])
tbl$tf_idf <- tbl$tf * tbl$idf
if(any(tbl$idf < 0, na.rm = TRUE)) {
rlang::warn(paste("A value for tf_idf is negative:\n",
"Input should have exactly one row per document-term combination."))
}
tbl
}
issue_tf_idf <- issue_words %>%
bind_tf_idf2(word, issue, n)
issue_tf_idf %>%
arrange(tf_idf)
bind_tf_idf2 <- function(tbl, term, document, n) {
term <- quo_name(enquo(term))
document <- quo_name(enquo(document))
n_col <- quo_name(enquo(n))
terms <- as.character(tbl[[term]])
documents <- as.character(tbl[[document]])
n <- tbl[[n_col]]
doc_totals <- tapply(n, documents, sum)
test3 <- data.frame(documents, terms, n, doct_totals)
idf <- log(length(doc_totals) / table(terms))
tbl$tf <- n / as.numeric(doc_totals[documents])
tbl$idf <- as.numeric(idf[terms])
tbl$tf_idf <- tbl$tf * tbl$idf
if(any(tbl$idf < 0, na.rm = TRUE)) {
rlang::warn(paste("A value for tf_idf is negative:\n",
"Input should have exactly one row per document-term combination."))
}
tbl
}
issue_tf_idf <- issue_words %>%
bind_tf_idf2(word, issue, n)
bind_tf_idf2 <- function(tbl, term, document, n) {
term <- quo_name(enquo(term))
document <- quo_name(enquo(document))
n_col <- quo_name(enquo(n))
terms <- as.character(tbl[[term]])
documents <- as.character(tbl[[document]])
n <- tbl[[n_col]]
doc_totals <- tapply(n, documents, sum)
test3 <- data.frame(documents, terms, n, doc_totals)
idf <- log(length(doc_totals) / table(terms))
tbl$tf <- n / as.numeric(doc_totals[documents])
tbl$idf <- as.numeric(idf[terms])
tbl$tf_idf <- tbl$tf * tbl$idf
if(any(tbl$idf < 0, na.rm = TRUE)) {
rlang::warn(paste("A value for tf_idf is negative:\n",
"Input should have exactly one row per document-term combination."))
}
tbl
}
issue_tf_idf <- issue_words %>%
bind_tf_idf2(word, issue, n)
issue_tf_idf <- issue_words %>%
bind_tf_idf(word, issue, n)
issue_tf_idf %>%
arrange(tf_idf)
term <- quo_name(enquo(word))
document <- quo_name(enquo(issue))
doc_totals <- tapply(issue_words$n, issue_words$issue, sum)
idf <- log(length(doc_totals) / table(issue_word$words))
doc_totals <- tapply(issue_words$n, issue_words$issue, sum)
idf <- log(length(doc_totals) / table(issue_words$words))
doc_totals <- tapply(issue_words$n, issue_words$issue, sum)
idf <- log(length(doc_totals) / table(issue_words$word))
doc_totals <- tapply(issue_words$n, issue_words$issue, sum)
idf <- log(length(doc_totals) / table(issue_words$word))
doc_totals <- tapply(issue_words$n, issue_words$issue, sum)
idf <- log(length(doc_totals) / table(issue_words$word))
idf[$word]
doc_totals <- tapply(issue_words$n, issue_words$issue, sum)
idf <- log(length(doc_totals) / table(issue_words$word))
idf
library(readr)
data_file_name <- '../data/times_ocr=80_100&date=1945-01-01_2010-12-31&query=_european_union_&category=News.csv'
data_df <- read_delim(data_file_name, delim = ";", escape_double = FALSE, col_types = cols(`date-pub` = col_date(format = "%B %d, %Y")), trim_ws = TRUE)
print(colnames(data_df))
View(data_df)
View(data_df)
library(tidyverse)
library(tidytext)
tidy_content <- data_df %>% unnest_tokens(word, content)
View(tidy_content)
library(dplyr)
library(tidytext)
issue_words <- data_df %>%
unnest_tokens(word, content) %>%
count(issue, word, sort = TRUE)
View(issue_words)
View(issue_words)
View(issue_words)
View(issue_words)
total_words <- issue_words %>%
group_by(issue) %>%
summarize(total = sum(n))
issue_total_words <- left_join(issue_words, total_words) %>% arrange(desc(issue))
View(total_words)
View(total_words)
View(issue_total_words)
View(issue_total_words)
issue_tf_idf <- issue_words %>%
bind_tf_idf(word, issue, n)
issue_tf_idf %>%
arrange(tf_idf)
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(tidyverse)
library(tidytext)
library(dplyr)
data_file_name <- '../data/times_ocr=80_100&date=1945-01-01_2010-12-31&query=_european_union_&category=News.csv'
data_df <- read_delim(data_file_name, delim = ";", escape_double = FALSE, col_types = cols(`date-pub` = col_date(format = "%B %d, %Y")), trim_ws = TRUE)
setwd("/Volumes/Samsung_T5/uu_job/workshops/workshop-text-mining-rsa")
library(readr)
library(tidyverse)
library(tidytext)
library(dplyr)
data_file_name <- '../data/times_ocr=80_100&date=1945-01-01_2010-12-31&query=_european_union_&category=News.csv'
data_df <- read_delim(data_file_name, delim = ";", escape_double = FALSE, col_types = cols(`date-pub` = col_date(format = "%B %d, %Y")), trim_ws = TRUE)
setwd("/Volumes/Samsung_T5/uu_job/workshops/workshop-text-mining-rsa/book")
library(readr)
library(tidyverse)
library(tidytext)
library(dplyr)
data_file_name <- '../data/times_ocr=80_100&date=1945-01-01_2010-12-31&query=_european_union_&category=News.csv'
data_df <- read_delim(data_file_name, delim = ";", escape_double = FALSE, col_types = cols(`date-pub` = col_date(format = "%B %d, %Y")), trim_ws = TRUE)
setwd("/Volumes/Samsung_T5/uu_job/workshops/workshop-text-mining-rsa/book")
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(tidyverse)
library(tidytext)
library(dplyr)
data_file_name <- '../data/times_ocr=80_100&date=1945-01-01_2010-12-31&query=_european_union_&category=News.csv'
data_df <- read_delim(data_file_name, delim = ";", escape_double = FALSE, col_types = cols(`date-pub` = col_date(format = "%B %d, %Y")), trim_ws = TRUE)
getwd()
library(readr)
library(tidyverse)
library(tidytext)
library(dplyr)
data_file_name <- '../data/times_ocr=80_100&date=1945-01-01_2010-12-31&query=_european_union_&category=News.csv'
data_df <- read_delim(data_file_name, delim = ";", escape_double = FALSE, col_types = cols(`date-pub` = col_date(format = "%B %d, %Y")), trim_ws = TRUE)
library(readr)
library(tidyverse)
library(tidytext)
library(dplyr)
data_file_name <- '../data/times_ocr=80_100&date=1945-01-01_2010-12-31&query=_european_union_&category=News.csv'
data_df <- read_delim(data_file_name, delim = ";", escape_double = FALSE, col_types = cols(`date-pub` = col_date(format = "%B %d, %Y")), trim_ws = TRUE)
issue_words <- data_df %>%
unnest_tokens(word, content) %>%
count(issue, word, sort = TRUE)
issue_tf_idf <- issue_words %>%
bind_tf_idf(word, issue, n)
issue_tf_idf %>%
arrange(tf_idf)
