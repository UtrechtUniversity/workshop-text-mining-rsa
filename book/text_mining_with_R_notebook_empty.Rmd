---
title: "Notes on Text-Mining course by RDM"
author: "best student ever"
output: html_document
date: "2023-05-14"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# My Notes

## This is a smaller header

### And so on

- This is a list item with a **bold** word;
- This is a list item with an *italic* word

# Setup

```{r}
#| eval: false
install.packages("tidyverse")
install.packages("tidytext")
install.packages("wordcloud")
```

```{r}
library(tidyverse)
library(tidytext)
library(wordcloud)
```

# Reading data

<!-- Determine your raw data file full or relative path -->
<!-- Read your file using the function read_delim(). Find the right arguments clicking on "Import Dataset" in the Environment tab -->
<!-- Print column names and number of rows using the functions colnames() and nrow() respectively -->

```{r}
data_file_name <- ???

data_df <- read_delim(???, 
    delim = ???)

print(???(data_df))
print(???(data_df))
```

# Tokenization

<!-- Using data_df as input data, use unnest_tokens() to tokenize your data into words -->
<!-- The first argument of unnest_tokens() is the name of the column that WILL cnotain the token -->
<!-- The second argument of unnest_tokens() is the name of the column containing the text to tokenize -->
<!-- The third argument of unnest_tokens() is the kind of token to create between double quotes -->

```{r}
tidy_content <- ??? %>% 
  unnest_tokens(word, ???, token= ???)
```

# Cleaning up data

<!-- Apply the function is.na to the column "issue" of the DataFrame "tidy_content" -->
<!-- Apply the function any() to the result of the previous operation -->
<!-- Print the result -->

```{r}
are_there_na <- ??? %>%
  ??? %>%
  ???
???
```

<!-- Select only the "tidy_content" rows whose issue values are NOT NA -->
<!-- Store the result in a variable called, again, "tidy_content" -->

```{r}
??? <- tidy_content[ ??? , ??? ]
```

<!-- Check again if the column issue contains any NA -->

```{r}
???
```

# Counting words 1

<!-- Using "tidy_content" as input, count the number of distinct words using the function count() -->
<!-- count() argument is the column name containing words. The counts will be automatically stored -->
<!-- in a new column named "n" -->
<!-- Use the function filter so to select words occuring more than 2000 times --!>
<!-- Rearrange the word column in order of descending n -->
```{r}
word_count <- ??? %>%
  count(???) %>%
  filter(???) %>%
  mutate(word = reorder(word, n)) 
```

<!-- Make a bar plot using "word_count" as input, n as x, and word as y -->
<!-- Display the plot -->
```{r}
word_count_plot <-
  ??? %>%
  ggplot(aes(???, ???)) +
  geom_col() +
  labs(y = NULL)
???
```

# Removing stop words

<!-- Load the stop_words DataFrame with data() -->
<!-- Filter out stop_words from the DataFrame "tidy_content" using anti_join() -->

```{r, warning=FALSE, message=FALSE}
data(stop_words)

tidy_clean_content <- ??? %>% 
  anti_join(???)

tidy_clean_content
```

# Counting words 2

<!-- Repeat the steps of counting words 1 for "tidy_clean_content" -->

```{r}
???
```

# Word cloud visualization

<!-- Use the function wordcloud() to produce a word cloud plot -->
<!-- First and second argument of wordcloud() are words and their frequencies, respectively -->
<!-- Use the DataFrame word count -->

```{r}
word_cloud_plot <-
  wordcloud( ??? , ??? )

word_cloud_plot
```

# Sentiment Analysis

## Lexicon and joy words

<!-- Load the NRC lexicon -->
<!-- Make a new DataFrame only with joy words -->

```{r}

```
# Computing joy words fraction

<!-- Compute the total number of words per issue between two arbitrary dates -->
<!-- Compute the total number of joy words per issue between two arbitrary dates -->
<!-- Merge the two DataFrames in a single one -->

```{r}

```

<!-- Plot the percent of joy words per issue -->

```{r}

```

# Computing total joy fraction

<!-- Make a DataFrame of distinct words -->
<!-- Count the number of distinct words -->
<!-- Count the number of distinct joy words -->
<!-- Compute the ratio between the two -->

```{r}

```

# Analyzing word and document frequency: tf-idf

## Computing term frequency

<!-- Count the number of distinct words per issue -->
<!-- Clean up the Dataframe from NA values -->
<!-- Count the total number of words per issue -->
<!-- Join the two previous DataFrames -->

```{r, warning=FALSE}

```

<!-- Make a DataFrame of issues with more than 10000 words -->
<!-- Select the first 6 items -->
<!-- Join the just created DataFrame with issue_total_words -->
<!-- Plot term frequency per issue -->

```{r}

```

## Computing and displaying tf-idf

<!-- Compute tf-idf with bind_tf_itg -->

```{r}

```

```{r}

```

<!-- Plot tf-idf -->

```{r}

```

# Relationships Between Words

<!-- Tokenize text in groups of 2 words -->

```{r}
```

```{r}

```

## Cleaning up biagrams

<!-- Cleanup your biagrams by stop words -->

```{r}

```

## Plotting biagrams

```{r}

```
